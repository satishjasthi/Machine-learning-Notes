*** Source: Udacity ***

Supervised learning is like a induction function where the machine learns by observing examples which are labeled as right and wrong.(induction basically mean identifying the patterns first and then creating a function which can generalise that pattern to the future inputs)

*Reinforcement learning is like learning from experience

*OPTIMIZATIONS PROBLEMS IN 
1. Supervised learning: to label data well
2. Unsupervised learning: to cluster data well
3. Reinforcement learning: to get best behaviour scores


TERMS IN CLASSIFICATION:
Instances: Input
Concept: an idea of mapping input to output
target concept: it is the actual required function which mapps from inputs to outputs
Hypothesis class : is a set of all possible functions
Sample: training set
candidate: Concept that can be a target concept


DECISION TREES:

		O
	   0	     0
	0     0   0     0

This represents the decision tree, the circle represents the decision nodes

Decision tree always moves from top to bottom

The goal of asking questions in Decision trees is to narrow down the space to a particular point


Decision tree learning:
1.Pick the best attribute(best is the one that narrows the scope by a greater amount )
2. Ask a question about it
3.Follow the answer path
4.Go to 1
 repeate 1 to 4 until you end up with what you want


Decision tree is linear if the number of nodes is equal to number of levels of tree

If the tree grows exponentially then decision tree is exponential

Information gain:is simply a mathematical way to capture the amount of information gain that we want to gain by picking particular attribute.
In other words information gain reduces the amount of randomness when selecting a specific attribute.Maximizing information gain means reducing the amount of randomness while choosing the attributes.

Gain(S,A) = Entropy(S) - SUM(v){(|Sv|/|S|)*Entropy(Sv)}
where S: is the collection of training examples
A: is a particular attribute
V:labels of attributes

entropy is maximum when the mixture has all impurities in equall proportions.And it is least when we have no impurities, ie we have only one category.


Entropy of a value = -Sum(from 1 to v){p(V)*log(P(V))}

Once we have understand about information gain and entropy which helps us understand how exactly a decision tree splits.

Now when we have n attributes for a given predictor then we can have n decision  trees possible, but how can we choose the optimal tree out of these n trees. For which we use
something called BIAS

We have 2 kinds of BIAS:
1.Restricition BIAS(H): it is a set which includes only functions that are decision trees.ie the mapping between X & Y can be done by any function(infinetly many) but we want to consider only the subset of them which are purely decision trees.

2.Reference BIAS(h): is a subset of H,ie out of possible decision trees which one are we choosing.

INDUCTIVE BIAS:(which is a reference bias)
what makes a good decision tree:
1. Good splits at top
2.Correct over incorrect
3.Shorter tree are better than longer trees

We can provide continuous attributes to decision tree as inputs

When do we stop spliting decision tree?
1. We can use PRUNING method
2. Use CROSS VALIDATION METHOD to find the decision tree that has least CV error


LINEAR REGRESSION:
Cross validation set: is a part of training set which is used to test whether a chosen model is overfitting or perform well.



*.Naive Bayes algorithm: 
*. It's mainly used for classification problems related to texts.
*.It is called Naive Bayes because it doesn't actually understand the sentences but it classifies them purely on the basis of their frequency.
Pros of NB algo: it's computationally cheap for large datasets
 

SUPPORT VECTOR MACHINES(SVM):
*.It basically creates a line between two class of the data ,something like a decision boundary but in this case it is called hyperplane

*. In SVM margin is nothing but the minimum distance between the hyperplane and the two class of the data it separates

|    *---|  *
|*  *    |
| *      |--*       ---- these distances are the margins
|  *     |  * 
|*       |  * 
| **     | * 
|________|______

*.SVM gives first preference to correct classification and then maximizes the margin

*. Adding new features to SVM can help it to linearly classify non linear data

*.SVM is also called large margin classifiers & it is highly sensitive to outliers when c or 1/lambda(regularization parameter) is very high( because it tries to minimize margin)

*. SVM and Kernels work together in a better way because SVM is mathematically & computationally optimised to use Kernels, eventhough you can use kernels with linear regression ,we won't use because it makes the algo slow & computationally expensive.

SVM PARAMETERS: C or 1/lambda
1Kernels: 

2.C:
2.1.Large C value : lower bias, high variance  (because lambda is small)
2.2.Small C value : Higher bias, low variance  (because lambda is large)

3.While using Gaussian kernel:
-Large sigma**2 value :Features fi vary more smoothly
higher bias,lower variance
-Small sigma**2 value: Features fi vary less smoothly .
Lower bias,high variance.

4.gamma parameter also plays a vital role in high variance & high bias

SVM is not a good algo to go when the dataset is really large and filled with a lot of noise,because it would be computationally expensive and very slow that is where we can use Naive Bayes algo.


OUTLIERS:
there are 2 kinds of them:
1. which we can ignore since they'll be caused by data entry error or sensor malfunctioning
2.which we have to pay special attention to them because they can give a vital anomaly in data.

*.Outlier removal:
1.Train on data
2.Remove points with largest residual error,fraction of data that would be removed will be around 10%.(but it vary depending on applications)
3.re-train the model on remaining data

*. Identifying and cleaning away outliers is something you should always think about when looking at a dataset for the first time

Neural Networks:
*.In order to train a NN you can use either:
1.Percepton rule or
2.Gradient descent

single neuron/perceptron: acts like a activator

for example X1,X1,X3 are inputs and W1,W2,W3 are respective weights and theta is the threshold value of the perceptron.
So if X1W1 + X2W2 + X3W3 >= theta then the perceptron will give 1 else 0 as output

In general 
if a = SUM(i = 1 to k){XiWi}

Y = 1 if a>=theta
Y = 0 if a< theta

perceptron are linear functions which always creates hyperplanes as decision boundaries

To find the optimal weights for perceptrons using training data set we have two methods:

1.perceptron rule
2.Gradient descent rule

1.Percepton rule: can be used when the data is linearly seperable(ie they can be completely separated by a straight line or a plane.)


Wi = Wi + dWi // Wi's are the weights and dWi is delta Wi
dWi = alpha * (y - y^)Xi // y is the actual output & y^ is the predicted value and Xi is the individual X values,aplha- learning rate

y^ = (SUM(i=1 to n){WiXi} >= 0)

Perceptron method is really good for linearly separable data,because it can find the solution in very few steps.

when we are using gradient descent in NN to train the model we use a differential function like sigma to make decision between 0 & 1. Activation values are feed to sigmoid function to get Y^ between 0 & 1.

Backpropagation in NN: is nothing but passing the errors in weights back to the NN to reduce them & therby improving ouput.

Gradient descent suffers from local optimum: to free it from this we have some methods for optimizing weights:
*momentum: use the concepts of momentum
* higher order derivatives
* randomization optimization
* penalty for complexity: model become complex:
	when we add more number of nodes,layers or large weights


INSTANCE BASED LEARNING:

KNN algo:





UNSUPERVISED LEARNING:
======================

*Clustering:
	Single link clustering(SLC):
How does SLC works: 
1. given a set of N points
2. Initiate the algorithm by considering each point as a single cluster
3. now calculate the distance between each of these clusters, those clusters which are nearer to each other will merge into a single cluster.
4.repeat 3 until N-K times,till you reach K clusters, where k is the number of clusters we need

SLC always considers the closest distance as a distance metric.

SLC is determinitic







=========================================================================
Source: Udacity
INTRO TO ALGORITHMS:

See IMAGE PD1

Nodes: are the vertices of the graph like SS,DH,JR
Edges/links: are the connections between the nodes
degree of a node: is the number of edges connected to a node
in image PD1 DH node has a degree of 4

Eulerian Path: is the path covered when you move from one node to other in a graph without going through the same path twice.


IMAGE PD2 shows the eulerian path and degree of nodes

It can be seen from the PD2, that all the nodes in the graph except starting and ending nodes have even degree. whereas starting and ending nodes has odd degree.

When you have starting and ending points at two different points,then the above rule applies

But if ending and starting point are the same then it is called Eulerian Tour which is a special kind of Eulerian path


Why do we need Algos:
* to get a quick response from the programs and applications that we run
* It basically helps your programm to fly instead of crawing using its clever techniques and math
* to optimise your program so that it doesn't waste time on unnecessary things

 
How do math helps us in learning algos?
1.It formalises what you want to do
2.analyse the correctness of your solution
3. Analyse the solution's efficiency(it refers to many like time, memory,energy usage)

Bitwise operators in python:
x>>y : means dividing x by 2**y or we can say that it half the number if it is even. if the number is odd then subtract 1 from it and half the number
x<<y : means multiply x by 2**y,this doubling number

Measuring the time taken by a algo using following assumptions as rules:
1.Simple statements takes unit time
Ex: x += 1

2. Sequence of simpe statements = the sum of unit time for each statement

Ex: if y=4; z = z/2 , takes 2 unit time

3.Loop takes time equal to the body x iterations

Ex: for i in range(4):
	print hello
this program takes 4 unit times


 














================================================
PROJECT: FINDING DONORS

DATA TRANSFORMATION IN STATISTICS:
Guidance for how data should be transformed, or whether a transformation should be applied at all, should come from the particular statistical analysis to be performed. For example, a simple way to construct an approximate 95% confidence interval for the population mean is to take the sample mean plus or minus standard error units. However, the constant factor 2 used here is particular to the normal distribution, and is only applicable if the sample mean varies approximately normally. The central limit theorem states that in many situations, the sample mean does vary normally if the sample size is reasonably large. However, if the population is substantially skewed and the sample size is at most moderate, the approximation provided by the central limit theorem can be poor, and the resulting confidence interval will likely have the wrong coverage probability. Thus, when there is evidence of substantial skew in the data, it is common to transform the data to a symmetric distribution before constructing a confidence interval. If desired, the confidence interval can then be transformed back to the original scale using the inverse of the transformation that was applied to the data.

Data can also be transformed to make it easier to visualize them. For example, suppose we have a scatterplot in which the points are the countries of the world, and the data values being plotted are the land area and population of each country. If the plot is made using untransformed data (e.g. square kilometers for area and the number of people for population), most of the countries would be plotted in tight cluster of points in the lower left corner of the graph. The few countries with very large areas and/or populations would be spread thinly around most of the graph's area. Simply rescaling units (e.g., to thousand square kilometers, or to millions of people) will not change this. However, following logarithmic transformations of both area and population, the points will be spread more uniformly in the graph.

A final reason that data can be transformed is to improve interpretability, even if no formal statistical analysis or visualization is to be performed. For example, suppose we are comparing cars in terms of their fuel economy. These data are usually presented as "kilometers per liter" or "miles per gallon." However, if the goal is to assess how much additional fuel a person would use in one year when driving one car compared to another, it is more natural to work with the data transformed by the reciprocal function, yielding liters per kilometer, or gallons per mile.


Precision and recall:
precision (also called positive predictive value) is the fraction of retrieved instances that are relevant, while recall (also known as sensitivity) is the fraction of relevant instances that are retrieved. Both precision and recall are therefore based on an understanding and measure of relevance.

In other words precision is how many of them are correct and recall is how many of them are correct out of what algo thought as correct

Precision = TP/(TP+FP)
recall = TP/(TP+FN) ,out of all correct ones how many can it recall.




Bayesian learning:
* learn the best hypothesis given data and some domain knowledge

* Bayes rule: 
	P(D|h)*P(h)
P(h|D) = ---------------
	P(D)
h - hypothesis; D - given some data 

P(D) - is the prior or the data
P(D|h) - labeling data given hypothesis,which is nothing but ML algo itself.
P(h) - is the prior(domain knowledge) about hypothesis,that which hypothesis is best sutiated out of all available ones


* how does it work:
for each h that belongs to H
calculate P(h|D) = P(D|h)*P(H)/P(D)

output : h = argmax P(h|D)...3.0, select the one with the highest prob

we can actually approximate P(h|D) = P(D|h)*P(H),
by negelecting P(D) ,because any way we are looking for max value of h in 3.0 and P(D) doesn't matter much, in most of the cases it is not possible to obtain prior on data.

So ,P(h|D) ~= P(D|h)*P(H) ...3.1
when we are using 3.1 we call it MAP(maximum a posterior),because here we have a porior for hypothesis.That here we give more importance to some hypothesis rather than other.

However in ML(maximum liklehood),we have no prior for hypothesis,because we assume that all hypothesis has same chances of being selected,in which

P(h|D) ~= P(D|h), with no priors for hypothesis

unless number of hypothesis(h) are fewer or limited this approach would be impractical

Bayes learning in action:
* 3 major assumptions:
	*given data points {(Xi,di)} as noise free examples of some function c, here noise free means all the points in the data can be expressed using function c
	* C belongs to(e) our Hypothesis set
	* all h's e H are equally likely

* know given a hypothesis how can we know the probability of a chosing that particular hypothesis given data

we know 

	P(D|h)*P(h)
P(h|D) = ---------------
	P(D)
P(h) = 1/|H|, |H| is the size of hypothesis set, which contains all possible hypothesis.

now P(D|h) = 1 if di = h(Xi) for all Xi,di e D ie dataset , nothing but predicted == actual value of independent variable

P(D|h) = 0 otherwise

P(D) = Sum(from h0 to hN){P(D|hi)*P(hi)}

Since we consider that all examples folloe function c

P(D) = Sum(hi e Versionspace,D){1 * (1/|H|)} = versionspace/|H|


P(h|D) = 1/versionspace ,it is only true for h e version space
 
Consider an Example:(https://classroom.udacity.com/nanodegrees/nd009/parts/596c7dc6-8049-4785-adfe-7c83ca19b00f/modules/5c2f3b47-b791-46a7-88eb-34a0753665e6/lessons/5462070314/concepts/4733385560923#)

dataset:
X	d
1	5
3	6
11	11
12	36
20	100

here d is the output value
let's say d = k*x, where k e {1,2,3....}
let the candid hypothesis be an identity function

h(x) = x

let the prob of prediciting each of the di is 1/2**k

then P(1/2**k) = 1/2**5 * 1/2**2 * 1/2 * 1/2**3 * 1/2**5 = 1/65536

Minimum Description length

We know h_map =  argmax P(D|h)*P(h)
	      =  argmax[lg(P(D|h) + lg(P(h))]
	      =  argmin[-lg(P(D|h) - lg(P(h))]...3.4
to max h_map we must min 3.4

-lg(P(D|h) ~ length(D|h)
-lg(h) ~ length(h), this is nothing but the min value of lg(h) gives the optimal hypothesis for given data

Bayes Inference:

Conditional Independence:
X is conditionally independent of Y given Z, if the probability distribution governing X is independent of the values of Y given the values of Z, that is if

for all X,Y,Z P(X=x|Y=y, Z=z) = P(X=x| Z=z)
ie P(X|YZ) = P(X|Z)

which is similar to 
P(X,Y) = P(X)*P(Y) if X & Y are independent

P(X,Y) = P(X|Y).P(Y)
p(X|Y) = P(X)


Belief Networks or Bayes Networks or Bayesian Networks or Graphical Models:






===================================================
ENSEMBLE LEARNING: BOOSTING

Ensemble learning algo's combines simple rules which cannot tackle the problem on their own but when they are combined together, they form a complex rule which can solve the problem efficaciously

========================================================================
Source:https://onlinecourses.science.psu.edu/stat414/node/97

Probability Density Functions:A continuous random variable takes on an uncountably infinite number of possible values. For a discrete random variable X that takes on a finite or countably infinite number of possible values, we determined P(X = x) for all of the possible values of X, and called it the probability mass function ("p.m.f."). For continuous random variables, as we shall soon see, the probability that X takes on any particular value x is 0. That is, finding P(X = x) for a continuous random variable X is not going to work. Instead, we'll need to find the probability that X falls in some interval (a, b), that is, we'll need to find P(a < X < b). We'll do that using a probability density function ("p.d.f."). 

Example
Even though a fast-food chain might advertise a hamburger as weighing a quarter-pound, you can well imagine that it is not exactly 0.25 pounds. One randomly selected hamburger might weigh 0.23 pounds while another might weigh 0.27 pounds.  What is the probability that a randomly selected hamburger weighs between 0.20 and 0.30 pounds? That is, if we let X denote the weight of a randomly selected quarter-pound hamburger in pounds, what is P(0.20 < X < 0.30)?

Now, you could imagine randomly selecting, let's say, 100 hamburgers advertised to weigh a quarter-pound. If you weighed the 100 hamburgers, and created a density histogram of the resulting weights, perhaps the histogram might look something like this: 

SEE IMAGE GD10


In this case, the histogram illustrates that most of the sampled hamburgers do indeed weigh close to 0.25 pounds, but some are a bit more and some a bit less. Now, what if we decreased the length of the class interval on that density histogram? Then, the density histogram would look something like this:


SEE IMAGE GD11

Now, what if we pushed this further and decreased the intervals even more? You can imagine that the intervals would eventually get so small that we could represent the probability distribution of X, not as a density histogram, but rather as a curve (by connecting the "dots" at the tops of the tiny tiny tiny rectangles) that, in this case, might look like this:

SEE IMAGE GD12

Such a curve is denoted f(x) and is called a (continuous) probability density function.

Now, you might recall that a density histogram is defined so that the area of each rectangle equals the relative frequency of the corresponding class, and the area of the entire histogram equals 1. That suggests then that finding the probability that a continuous random variable X falls in some interval of values involves finding the area under the curve f(x) sandwiched by the endpoints of the interval. In the case of this example, the probability that a randomly selected hamburger weighs between 0.20 and 0.30 pounds is then this area:


SEE IMAGE GD13

Now that we've motivated the idea behind a probability density function for a continuous random variable, let's now go and formally define it.


SEE IMAGE GD14 15 16


Reinforcement learning:(RL)
======================

In case of supervised learning y = f(x)
here given x ,ML algo will try to develop a function f which can predict y

In case of unsupervised learning f(x)
here given x, ML algo will try to find the function to cluster the data into similar category.

In case of RL y,z = f(x)
here given x & z, ML algo will try to find the function using x and z , to predict y


RL is one of the mechanisms to perform Decision Making.

Markov's Decision Process(MDP):
=======================
States:(S) are like a sample space which involves all possible states possible for any given situation.

Actions A(s) or A: are the things that we can perform in any given states.

Transition Model : T(s,a,s') ~ Prob(s'|s,a)
transition is like the physics that governs your actions or your entire play in any given world.
It consists of three parameters
s - initial state
a - action taken
s' - finial state, which can be same as initial state 

T(s,a,s') ~ Prob(s'|s,a) states that T(s,a,s') is the probability of you ending up at state s' given you have taken action 'a' from state 's' .

Markovian property: states that only the  present matters and the rules of the world are stationary.

Reward R(s),R(s,a),R(s,a,s'): is some scalar value that you'll get when you are in a particular state.


MDP is like a problem and the solution to this problem is something called "Policy"

Policy : PI(s) ----> a ie for any given state that you are in it gives you the action you need to perform.

Policy* or PI* is the optimal policy which maximises the long term expected reward

So if consider RL as Supervised learining it would look something like
<s1,a1>,<s2,a2>..........<sn,an> ie you are given n pairs of states and actions based on which the ML algo will devlop f which is PI in Rl

however what exactly happens in RL is you are given 
<s1,a1,r1>,<s2,a2,r2>..........<sn,an,rn>  ie you are given n pairs of states,actions & rewards based on which the RL algo will devlop f which is PI in Rl and the 'r' is equivalent 'z' that we have discussed in RL definiton.
ie y,z = f(x) 

Policy is the one that states you what action you must take at any given state you are in.

Policy is different from plan making because it doesn't tell you what actions you must take to reach the goal but it only states what immediate action you should take given your state

Temporal Credit assignment problem: is where your algo will learn to choose optimal actions from late rewards in contrast with immediate rewards like in your Supervised learning.

Ex: 
SEE IMAGE Temporal credit problem

here in case of SL you'll be rewarded at each state whereas in RL you'll be rewarded at reward state

The value of the reward is the one that plays a vital role in MDP to reach the final reward position as quickly as possible or as slowly as possible.


Sequence of rewards:

* In case of RL if we have infinite time to reach from start till goal then it is said to have inifinite horizon and policy for the game is defined as

Pi(s) ---> a ; ie Policy(state) = action
here the action wrt to a given state remains same no matter at what time you check it

See IMAGE game.png

here as we move from block 8--> 7---> 6 and so on till ---> 5---> Goal.
So if we check the state of block 7 at any point of time it remains the same <----


*Whereas in case of finite horizons the state changes wrt to time, in case of finite horizons markovian property of a state fails because the policy of the state doesn't remain constant it actually changes with time.

SEE IMAGE game.png

here as we move from block 8--> 7 the algorithm choose to enter block 9 instead of 6 , inspite of the danger of getting into Danger block because this is a finite horizon where the time left to complete the game decerease for every step you take
For example if you have only  4 sec left to complete the game and if you are in block 7 then the game choose the path 7-9-5-goal rather than 7-6-2-3--5-goal.
So the conceptof everything remains stationary fails in case of finite horizon.
hence in case of finite horizon 
Pi(s,t)---> a
t - time

Utility of Sequences: 
if we have two different sequences of states S0,S1,S2..... and S0,S'1,S'2....

if U{S0,S1,S2.....} > U{S0,S'1,S'2....}

then U{S1,S2.....} > U{S'1,S'2....}

where U - utility function

this Utility function is defined as 

U{S0,S1,S2.....} = SUM(from t=0 to Infinity){R(St)} = Infinity.....1.0

ie the value of this utility function is infinity because for any given sequence it rewards is as shown below:

Sequence : S0,S1,S2.....
Reward :   R1,R2,R3...... and so on

so if we keep on adding Rewards R for every sequence ,since seqeunce doesn't end the sum keep on increases and becomes infinity. 

Intutively: It is like whatever you do with your life you'll be rewarded and you'll be immortal, in other words you won't risk to improve yourself because no matter what you do you'll be rewarded.

But if we define U as below

U{S0,S1,S2.....}  = SUM(from t=0 to infinity){Gamma**t R(St)}...2.0

where 0 <= gamma <1
in other words the equation 2.0 is just the sum of terms upto infinity of geometric series, which is

U{S0,S1,S2.....} = (Rmax)* (1 / (1-gamma))....3.0

eq 3.0 is called discounted reward

if we observe 2.0, we can makeout that as t increases gamma value goes on decreasing(since gamma is a fraction less than 1) so the reward at each consecutive step decreases.

It is like covering infinite distance within finite time, that what eq 2.0 means


Policies:

We know optimal policy is defined as 

Pi* = argmax(Pi){E(R|Pi)}....4.0

 where R = SUM(from t=0 to infinity){gamma**t R(St)}, ie discounted reward

U{Pi}(S) is U with policy Pi and State S

U{Pi}(S) = E[R|Pi,S0 = S]...5.0

eq 5.0 represents Utility function for a state when you choosen Pi as a Policy. In other words it's indicating the long term rewards that you'll get if you have used the Policy Pi and start at state S0.

For example you've choosen a path(Policy) and the start state for your game and you'll play the game,then what would be the reward that you'll be getting at the end of the game is given by eq 5.0 

U(s) always returns delayed or long term rewards whereas R(S) returns the immediate rewards.So rewards and utility can never be the same.


so optimal policy

Pi*(s) = argmax(a){SUM(T(s,a,s')U(s'))}

where 
U(S) = R(S) + Gamma * Max(a){SUM(T(s,a,s')U(s'))}...6.0

eq 6.0 is called Bellman equation


Finding policies:
 we know from Bellman's equation

U(S) = R(S) + Gamma * Max(a){SUM(T(s,a,s')U(s'))} 

so we can use this eq as 

Start with some arbitrary utilities
Update utilities based on neighbors(ie any state that you can reach)
repeat until convergence

^U(S)(t+1) = R(S)  + Gamma Max(a){SUM(T(s,a,s')* ^Ut(s'))}


So I'll update ^U(S)(t+1), which is the utility of current state 

^Ut(s') is the Utility of all other remaining states.











