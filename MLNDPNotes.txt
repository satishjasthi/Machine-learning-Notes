*** Source: Udacity ***

Supervised learning is like a induction function where the machine learns by observing examples which are labeled as right and wrong.(induction basically mean identifying the patterns first and then creating a function which can generalise that pattern to the future inputs)

*Reinforcement learning is like learning from experience

*OPTIMIZATIONS PROBLEMS IN 
1. Supervised learning: to label data well
2. Unsupervised learning: to cluster data well
3. Reinforcement learning: to get best behaviour scores


TERMS IN CLASSIFICATION:
Instances: Input
Concept: an idea of mapping input to output
target concept: it is the actual required function which mapps from inputs to outputs
Hypothesis class : is a set of all possible functions
Sample: training set
candidate: Concept that can be a target concept


DECISION TREES:

		O
	   0	     0
	0     0   0     0

This represents the decision tree, the circle represents the decision nodes

Decision tree always moves from top to bottom

The goal of asking questions in Decision trees is to narrow down the space to a particular point


Decision tree learning:
1.Pick the best attribute(best is the one that narrows the scope by a greater amount )
2. Ask a question about it
3.Follow the answer path
4.Go to 1
 repeate 1 to 4 until you end up with what you want


Decision tree is linear if the number of nodes is equal to number of levels of tree

If the tree grows exponentially then decision tree is exponential

Information gain:is simply a mathematical way to capture the amount of information gain that we want to gain by picking particular attribute.
In other words information gain reduces the amount of randomness when selecting a specific attribute.Maximizing information gain means reducing the amount of randomness while choosing the attributes.

Gain(S,A) = Entropy(S) - SUM(v){(|Sv|/|S|)*Entropy(Sv)}
where S: is the collection of training examples
A: is a particular attribute
V:labels of attributes

entropy is maximum when the mixture has all impurities in equall proportions.And it is least when we have no impurities, ie we have only one category.


Entropy of a value = -Sum(from 1 to v){p(V)*log(P(V))}

Once we have understand about information gain and entropy which helps us understand how exactly a decision tree splits.

Now when we have n attributes for a given predictor then we can have n decision  trees possible, but how can we choose the optimal tree out of these n trees. For which we use
something called BIAS

We have 2 kinds of BIAS:
1.Restricition BIAS(H): it is a set which includes only functions that are decision trees.ie the mapping between X & Y can be done by any function(infinetly many) but we want to consider only the subset of them which are purely decision trees.

2.Reference BIAS(h): is a subset of H,ie out of possible decision trees which one are we choosing.

INDUCTIVE BIAS:(which is a reference bias)
what makes a good decision tree:
1. Good splits at top
2.Correct over incorrect
3.Shorter tree are better than longer trees

We can provide continuous attributes to decision tree as inputs

When do we stop spliting decision tree?
1. We can use PRUNING method
2. Use CROSS VALIDATION METHOD to find the decision tree that has least CV error


LINEAR REGRESSION:
Cross validation set: is a part of training set which is used to test whether a chosen model is overfitting or perform well.



*.Naive Bayes algorithm: 
*. It's mainly used for classification problems related to texts.
*.It is called Naive Bayes because it doesn't actually understand the sentences but it classifies them purely on the basis of their frequency.
Pros of NB algo: it's computationally cheap for large datasets
 

SUPPORT VECTOR MACHINES(SVM):
*.It basically creates a line between two class of the data ,something like a decision boundary but in this case it is called hyperplane

*. In SVM margin is nothing but the minimum distance between the hyperplane and the two class of the data it separates

|    *---|  *
|*  *    |
| *      |--*       ---- these distances are the margins
|  *     |  * 
|*       |  * 
| **     | * 
|________|______

*.SVM gives first preference to correct classification and then maximizes the margin

*. Adding new features to SVM can help it to linearly classify non linear data

*.SVM is also called large margin classifiers & it is highly sensitive to outliers when c or 1/lambda(regularization parameter) is very high( because it tries to minimize margin)

*. SVM and Kernels work together in a better way because SVM is mathematically & computationally optimised to use Kernels, eventhough you can use kernels with linear regression ,we won't use because it makes the algo slow & computationally expensive.

SVM PARAMETERS: C or 1/lambda
1Kernels: 

2.C:
2.1.Large C value : lower bias, high variance  (because lambda is small)
2.2.Small C value : Higher bias, low variance  (because lambda is large)

3.While using Gaussian kernel:
-Large sigma**2 value :Features fi vary more smoothly
higher bias,lower variance
-Small sigma**2 value: Features fi vary less smoothly .
Lower bias,high variance.

4.gamma parameter also plays a vital role in high variance & high bias

SVM is not a good algo to go when the dataset is really large and filled with a lot of noise,because it would be computationally expensive and very slow that is where we can use Naive Bayes algo.


OUTLIERS:
there are 2 kinds of them:
1. which we can ignore since they'll be caused by data entry error or sensor malfunctioning
2.which we have to pay special attention to them because they can give a vital anomaly in data.

*.Outlier removal:
1.Train on data
2.Remove points with largest residual error,fraction of data that would be removed will be around 10%.(but it vary depending on applications)
3.re-train the model on remaining data

*. Identifying and cleaning away outliers is something you should always think about when looking at a dataset for the first time

Neural Networks:
*.In order to train a NN you can use either:
1.Percepton rule or
2.Gradient descent

single neuron/perceptron: acts like a activator

for example X1,X1,X3 are inputs and W1,W2,W3 are respective weights and theta is the threshold value of the perceptron.
So if X1W1 + X2W2 + X3W3 >= theta then the perceptron will give 1 else 0 as output

In general 
if a = SUM(i = 1 to k){XiWi}

Y = 1 if a>=theta
Y = 0 if a< theta

perceptron are linear functions which always creates hyperplanes as decision boundaries

To find the optimal weights for perceptrons using training data set we have two methods:

1.perceptron rule
2.Gradient descent rule

1.Percepton rule: can be used when the data is linearly seperable(ie they can be completely separated by a straight line or a plane.)


Wi = Wi + dWi // Wi's are the weights and dWi is delta Wi
dWi = alpha * (y - y^)Xi // y is the actual output & y^ is the predicted value and Xi is the individual X values,aplha- learning rate

y^ = (SUM(i=1 to n){WiXi} >= 0)

Perceptron method is really good for linearly separable data,because it can find the solution in very few steps.

when we are using gradient descent in NN to train the model we use a differential function like sigma to make decision between 0 & 1. Activation values are feed to sigmoid function to get Y^ between 0 & 1.

Backpropagation in NN: is nothing but passing the errors in weights back to the NN to reduce them & therby improving ouput.

Gradient descent suffers from local optimum: to free it from this we have some methods for optimizing weights:
*momentum: use the concepts of momentum
* higher order derivatives
* randomization optimization
* penalty for complexity: model become complex:
	when we add more number of nodes,layers or large weights


INSTANCE BASED LEARNING:

KNN algo:





UNSUPERVISED LEARNING:
======================

*Clustering:
	Single link clustering(SLC):
How does SLC works: 
1. given a set of N points
2. Initiate the algorithm by considering each point as a single cluster
3. now calculate the distance between each of these clusters, those clusters which are nearer to each other will merge into a single cluster.
4.repeat 3 until N-K times,till you reach K clusters, where k is the number of clusters we need

SLC always considers the closest distance as a distance metric.

SLC is determinitic







=========================================================================
Source: Udacity
INTRO TO ALGORITHMS:

See IMAGE PD1

Nodes: are the vertices of the graph like SS,DH,JR
Edges/links: are the connections between the nodes
degree of a node: is the number of edges connected to a node
in image PD1 DH node has a degree of 4

Eulerian Path: is the path covered when you move from one node to other in a graph without going through the same path twice.


IMAGE PD2 shows the eulerian path and degree of nodes

It can be seen from the PD2, that all the nodes in the graph except starting and ending nodes have even degree. whereas starting and ending nodes has odd degree.

When you have starting and ending points at two different points,then the above rule applies

But if ending and starting point are the same then it is called Eulerian Tour which is a special kind of Eulerian path


Why do we need Algos:
* to get a quick response from the programs and applications that we run
* It basically helps your programm to fly instead of crawing using its clever techniques and math
* to optimise your program so that it doesn't waste time on unnecessary things

 
How do math helps us in learning algos?
1.It formalises what you want to do
2.analyse the correctness of your solution
3. Analyse the solution's efficiency(it refers to many like time, memory,energy usage)

Bitwise operators in python:
x>>y : means dividing x by 2**y or we can say that it half the number if it is even. if the number is odd then subtract 1 from it and half the number
x<<y : means multiply x by 2**y,this doubling number

Measuring the time taken by a algo using following assumptions as rules:
1.Simple statements takes unit time
Ex: x += 1

2. Sequence of simpe statements = the sum of unit time for each statement

Ex: if y=4; z = z/2 , takes 2 unit time

3.Loop takes time equal to the body x iterations

Ex: for i in range(4):
	print hello
this program takes 4 unit times


 














================================================
PROJECT: FINDING DONORS

DATA TRANSFORMATION IN STATISTICS:
Guidance for how data should be transformed, or whether a transformation should be applied at all, should come from the particular statistical analysis to be performed. For example, a simple way to construct an approximate 95% confidence interval for the population mean is to take the sample mean plus or minus standard error units. However, the constant factor 2 used here is particular to the normal distribution, and is only applicable if the sample mean varies approximately normally. The central limit theorem states that in many situations, the sample mean does vary normally if the sample size is reasonably large. However, if the population is substantially skewed and the sample size is at most moderate, the approximation provided by the central limit theorem can be poor, and the resulting confidence interval will likely have the wrong coverage probability. Thus, when there is evidence of substantial skew in the data, it is common to transform the data to a symmetric distribution before constructing a confidence interval. If desired, the confidence interval can then be transformed back to the original scale using the inverse of the transformation that was applied to the data.

Data can also be transformed to make it easier to visualize them. For example, suppose we have a scatterplot in which the points are the countries of the world, and the data values being plotted are the land area and population of each country. If the plot is made using untransformed data (e.g. square kilometers for area and the number of people for population), most of the countries would be plotted in tight cluster of points in the lower left corner of the graph. The few countries with very large areas and/or populations would be spread thinly around most of the graph's area. Simply rescaling units (e.g., to thousand square kilometers, or to millions of people) will not change this. However, following logarithmic transformations of both area and population, the points will be spread more uniformly in the graph.

A final reason that data can be transformed is to improve interpretability, even if no formal statistical analysis or visualization is to be performed. For example, suppose we are comparing cars in terms of their fuel economy. These data are usually presented as "kilometers per liter" or "miles per gallon." However, if the goal is to assess how much additional fuel a person would use in one year when driving one car compared to another, it is more natural to work with the data transformed by the reciprocal function, yielding liters per kilometer, or gallons per mile.


Precision and recall:
precision (also called positive predictive value) is the fraction of retrieved instances that are relevant, while recall (also known as sensitivity) is the fraction of relevant instances that are retrieved. Both precision and recall are therefore based on an understanding and measure of relevance.

In other words precision is how many of them are correct and recall is how many of them are correct out of what algo thought as correct

Precision = TP/(TP+FP)
recall = TP/(TP+FN) ,out of all correct ones how many can it recall.




Bayesian learning:
* learn the best hypothesis given data and some domain knowledge

* Bayes rule: 
	P(D|h)*P(h)
P(h|D) = ---------------
	P(D)
h - hypothesis; D - given some data 

P(D) - is the prior or the data
P(D|h) - labeling data given hypothesis,which is nothing but ML algo itself.
P(h) - is the prior(domain knowledge) about hypothesis,that which hypothesis is best sutiated out of all available ones


* how does it work:
for each h that belongs to H
calculate P(h|D) = P(D|h)*P(H)/P(D)

output : h = argmax P(h|D)...3.0, select the one with the highest prob

we can actually approximate P(h|D) = P(D|h)*P(H),
by negelecting P(D) ,because any way we are looking for max value of h in 3.0 and P(D) doesn't matter much, in most of the cases it is not possible to obtain prior on data.

So ,P(h|D) ~= P(D|h)*P(H) ...3.1
when we are using 3.1 we call it MAP(maximum a posterior),because here we have a porior for hypothesis.That here we give more importance to some hypothesis rather than other.

However in ML(maximum liklehood),we have no prior for hypothesis,because we assume that all hypothesis has same chances of being selected,in which

P(h|D) ~= P(D|h), with no priors for hypothesis

unless number of hypothesis(h) are fewer or limited this approach would be impractical

Bayes learning in action:
* 3 major assumptions:
	*given data points {(Xi,di)} as noise free examples of some function c, here noise free means all the points in the data can be expressed using function c
	* C belongs to(e) our Hypothesis set
	* all h's e H are equally likely

* know given a hypothesis how can we know the probability of a chosing that particular hypothesis given data

we know 

	P(D|h)*P(h)
P(h|D) = ---------------
	P(D)
P(h) = 1/|H|, |H| is the size of hypothesis set, which contains all possible hypothesis.

now P(D|h) = 1 if di = h(Xi) for all Xi,di e D ie dataset , nothing but predicted == actual value of independent variable

P(D|h) = 0 otherwise

P(D) = Sum(from h0 to hN){P(D|hi)*P(hi)}

Since we consider that all examples folloe function c

P(D) = Sum(hi e Versionspace,D){1 * (1/|H|)} = versionspace/|H|


P(h|D) = 1/versionspace ,it is only true for h e version space
 
Consider an Example:(https://classroom.udacity.com/nanodegrees/nd009/parts/596c7dc6-8049-4785-adfe-7c83ca19b00f/modules/5c2f3b47-b791-46a7-88eb-34a0753665e6/lessons/5462070314/concepts/4733385560923#)

dataset:
X	d
1	5
3	6
11	11
12	36
20	100

here d is the output value
let's say d = k*x, where k e {1,2,3....}
let the candid hypothesis be an identity function

h(x) = x

let the prob of prediciting each of the di is 1/2**k

then P(1/2**k) = 1/2**5 * 1/2**2 * 1/2 * 1/2**3 * 1/2**5 = 1/65536

Minimum Description length

We know h_map =  argmax P(D|h)*P(h)
	      =  argmax[lg(P(D|h) + lg(P(h))]
	      =  argmin[-lg(P(D|h) - lg(P(h))]...3.4
to max h_map we must min 3.4

-lg(P(D|h) ~ length(D|h)
-lg(h) ~ length(h), this is nothing but the min value of lg(h) gives the optimal hypothesis for given data

Bayes Inference:

Conditional Independence:
X is conditionally independent of Y given Z, if the probability distribution governing X is independent of the values of Y given the values of Z, that is if

for all X,Y,Z P(X=x|Y=y, Z=z) = P(X=x| Z=z)
ie P(X|YZ) = P(X|Z)

which is similar to 
P(X,Y) = P(X)*P(Y) if X & Y are independent

P(X,Y) = P(X|Y).P(Y)
p(X|Y) = P(X)


Belief Networks or Bayes Networks or Bayesian Networks or Graphical Models:






===================================================
ENSEMBLE LEARNING: BOOSTING

Ensemble learning algo's combines simple rules which cannot tackle the problem on their own but when they are combined together, they form a complex rule which can solve the problem efficaciously

========================================================================
Source:https://onlinecourses.science.psu.edu/stat414/node/97

Probability Density Functions:A continuous random variable takes on an uncountably infinite number of possible values. For a discrete random variable X that takes on a finite or countably infinite number of possible values, we determined P(X = x) for all of the possible values of X, and called it the probability mass function ("p.m.f."). For continuous random variables, as we shall soon see, the probability that X takes on any particular value x is 0. That is, finding P(X = x) for a continuous random variable X is not going to work. Instead, we'll need to find the probability that X falls in some interval (a, b), that is, we'll need to find P(a < X < b). We'll do that using a probability density function ("p.d.f."). 

Example
Even though a fast-food chain might advertise a hamburger as weighing a quarter-pound, you can well imagine that it is not exactly 0.25 pounds. One randomly selected hamburger might weigh 0.23 pounds while another might weigh 0.27 pounds.  What is the probability that a randomly selected hamburger weighs between 0.20 and 0.30 pounds? That is, if we let X denote the weight of a randomly selected quarter-pound hamburger in pounds, what is P(0.20 < X < 0.30)?

Now, you could imagine randomly selecting, let's say, 100 hamburgers advertised to weigh a quarter-pound. If you weighed the 100 hamburgers, and created a density histogram of the resulting weights, perhaps the histogram might look something like this: 

SEE IMAGE GD10


In this case, the histogram illustrates that most of the sampled hamburgers do indeed weigh close to 0.25 pounds, but some are a bit more and some a bit less. Now, what if we decreased the length of the class interval on that density histogram? Then, the density histogram would look something like this:


SEE IMAGE GD11

Now, what if we pushed this further and decreased the intervals even more? You can imagine that the intervals would eventually get so small that we could represent the probability distribution of X, not as a density histogram, but rather as a curve (by connecting the "dots" at the tops of the tiny tiny tiny rectangles) that, in this case, might look like this:

SEE IMAGE GD12

Such a curve is denoted f(x) and is called a (continuous) probability density function.

Now, you might recall that a density histogram is defined so that the area of each rectangle equals the relative frequency of the corresponding class, and the area of the entire histogram equals 1. That suggests then that finding the probability that a continuous random variable X falls in some interval of values involves finding the area under the curve f(x) sandwiched by the endpoints of the interval. In the case of this example, the probability that a randomly selected hamburger weighs between 0.20 and 0.30 pounds is then this area:


SEE IMAGE GD13

Now that we've motivated the idea behind a probability density function for a continuous random variable, let's now go and formally define it.


SEE IMAGE GD14 15 16




















































======================================================================
Source: JointProbabilities pdf file in ML notes folder in MLNDP folder
UNDERSTANDING JOINT AND MARGINAL PROBABILITY DISTRIBUTION:

Joint distribution allows us to compute probabilities of events involving both variables and understand the relationship between the
variables. This is simplest when the variables are independent. When they are not, we use covariance and correlation to measure of the nature of the dependence between them.



Joint Distribution:

3.1 Discrete case:

Suppose X and Y are two discrete random variables and that X takes values {x1, x2, . . . , xn} and Y takes values {y1, y2, . . . , ym}. The ordered pair (X, Y ) take values in the product {(x1, y1), (x1, y2), . . . (xn, ym)}. The joint probability mass function (joint pmf) of X and Y is the function p(xi, yj) giving the probability of the joint outcome X = xi, Y = yj.

We organize this in a joint probability table as shown:


X\Y 	y1 	y2 . . . 	yj . . . 	ym
x1  p(x1, y1)  p(x1, y2) · · · p(x1, yj) · · · p(x1, ym)
x2  p(x2, y1)  p(x2, y2) · · · p(x2, yj) · · · p(x2, ym)
· · · · · · · · · · · · · · · · · · · · ·
· · · · · · · · · · · · · · · · · · · · ·
xi  p(xi, y1) p(xi, y2) · · · p(xi, yj) · · · p(xi, ym)
· · · · · · · · · · · · · · · · · ·
xn  p(xn, y1) p(xn, y2) · · · p(xn, yj) · · · p(xn, ym)


Ex:Roll two dice. Let X be the value on the first die and let T be the total on both dice. Here is the joint probability table

X\T 2    3    4    5    6    7    8    9   10   11   12
1   1/36 1/36 1/36 1/36 1/36 1/36 0    0    0    0    0 
2   0    1/36 1/36 1/36 1/36 1/36 1/36 0    0    0    0
3   0    0    1/36 1/36 1/36 1/36 1/36 1/36 0    0    0
4   0    0    0    1/36 1/36 1/36 1/36 1/36 1/36 0    0
5   0    0    0    0    1/36 1/36 1/36 1/36 1/36 1/36 0
6   0    0    0    0    0    1/36 1/36 1/36 1/36 1/36 1/36


TWO MOST IMPORTANT PROPERTIES OF JOINT PROBABILITIES:

1. 0 ≤ p(xi, yj) ≤ 1
2. The total probability is 1. We can express this as a double sum:

SUM(from i = 1 to n)SUM(from j = 1 to m){p(xi,yj)} = 1


3.2 Continuous case:
The continuous case is essentially the same as the discrete case: we just replace discrete sets of values by continuous intervals, the joint pmf by a joint pdf, and sums by integrals.

If X takes values in [a, b] and Y takes values in [c, d] then the pair (X, Y ) takes values in the product [a, b] × [c, d]. The joint probability density function (joint pdf) of X and Y is a
function f(x, y) giving the probability density at (x, y). That is, the probability that (X, Y ) is in a small rectangle of width dx and height dy around (x, y) is f(x, y) dx dy

       y^
	|		   |---------------->Prob = f(x,y)dx dy
      d-|	-----------|-------------
	|	|          |             |
	|	|        ----            |
	|	|       |    |           |
	|	|       |    | dy        |
	|	|        ----            |
	|	|          dx            |
	|	|                        |
      c-|	-------------------------
	|
----------------------------------------------->x
           '                             '
	   a  				 b
A joint probability density function must satisfy two properties:
1. 0 ≤ f(x, y)
2. The total probability is 1. We now express this as a double integral:

Integral(from c to d)Integral(from a to b){f(x,y)dxdy} = 1


Note: as with the pdf of a single random variable, the joint pdf f(x, y) can take values greater than 1; it is a probability density, not a probability

3.3 Events
Random variables are useful for describing events. Recall that an event is a set of outcomes and that random variables assign numbers to outcomes. For example, the event ‘X > 1’ is the set of all outcomes for which X is greater than 1. These concepts readily extend to pairs of random variables and joint outcomes.


Example 3. In Example 1, describe the event B = ‘Y − X ≥ 2’ and find its probability.
answer: We can describe B as a set of (X, Y ) pairs:
B = {(1, 3), (1, 4), (1, 5), (1, 6), (2, 4), (2, 5), (2, 6), (3, 5), (3, 6), (4, 6)}.

The probability of B is the sum of the probabilities in the orange shaded squares, so
P (B) = 10/36.



























*** Source: Bishop-Pattern recognition and Machine learning text book ***


*The ability to categorize correctly new examples that differ from those used for training is known as generalization.

*Feature extraction: is a process of extracting all the vital information(potential features) from inputs to improve the performance of the ML algorithm to predict the output.

Ex: For instance, in the digit recognition problem, the images of the digits are typically translated and scaled so that each digit is contained within a box of a fixed size. This greatly reduces the variability within each digit class, because the location and scale of all the digits are now the same, which makes it much easier for a subsequent pattern recognition algorithm to distinguish between the different classes. 

feature extraction is a part of pre-processing

*Pre-processing might also be performed in order to speed up computation. For example, if the goal is real-time face detection in a high-resolution video stream, the computer must handle huge numbers of pixels per second, and presenting these directly to a complex pattern recognition algorithm may be computationally infeasible. Instead, the aim is to find useful features that are fast to compute, and yet that also preserve useful discriminatory information enabling faces to be distinguished
from non-faces. These features are then used as the inputs to the pattern recognition algorithm.

*Applications in which the training data comprises examples of the input vectors along with their corresponding target vectors are known as supervised learning problems. Cases such as the digit recognition example, in which the aim is to assign each input vector to one of a finite number of discrete categories, are called classification problems. If the desired output consists of one or more continuous variables, then
the task is called regression. An example of a regression problem would be the prediction of the yield in a chemical manufacturing process in which the inputs consist
of the concentrations of reactants, the temperature, and the pressure.

*the technique of reinforcement learning (Sutton and Barto, 1998) is concerned with the problem of finding suitable actions to take in a given situation in
order to maximize a reward. Here the learning algorithm is not given examples of optimal outputs, in contrast to supervised learning, but must instead discover them
by a process of trial and error. 

A general feature of reinforcement learning is the trade-off between exploration, in which the system tries
out new kinds of actions to see how effective they are, and exploitation, in which the system makes use of actions that are known to yield a high reward. Too strong
a focus on either exploration or exploitation will yield poor results.


Probability theory:
i = 1 to M
j = 1 to L

nij = Consider a total of N trials in which we sample both of the variables X and Y , and let the number of such trials in which X = xi and Y = yj be nij.

cij = is the number of trials in which X takes the value xi (irrespective of the value that Y takes)

rij =  is the same case with y

The probability that X will take the value xi and Y will take the value yj is written p(X = xi, Y = yj) and is called the joint probability of X = xi and Y = yj. It is given by the number of points falling in the cell i,j as a fraction of the total number of points, and hence

p(X = xi, Y = yj) = nij/N


Similarly, the probability that X takes the value xi irrespective of the value of Y is written as p(X = xi) and is given by the fraction of the total number of points that fall in column i, so that

p(X = xi) = ci/N

or in general we can write

p(X = xi)  = Sum(j=1 to L){p(X = xi,Y=yj) }


which is the sum rule of probability. Note that p(X = xi) is sometimes called the marginal probability, because it is obtained by marginalizing, or summing out, the
other variables (in this case Y ).


If we consider only those instances for which X = xi, then the fraction of such instances for which Y = yj is written p(Y = yj|X = xi) and is called the conditional probability of Y = yj given X = xi. It is obtained by finding the
fraction of those points in column i that fall in cell i,j and hence is given by


p(Y = yj|X = xi) = nij/ci = P(X&Y)/P(X).

we can then derive the following relationship,which is the product rule
p(X = xi, Y = yj) = nij/N = nij/ci *ci/N = p(Y = yj|X =xi)*p(X = xi)


From the product rule, together with the symmetry property p(X, Y ) = p(Y, X)

p(Y |X) = p(X|Y )p(Y)/p(X)....1.12

which is called Bayes’ theorem and which plays a central role in pattern recognition and machine learning. Using the sum rule, the denominator in Bayes’ theorem can
be expressed in terms of the quantities appearing in the numerator


p(X) = Sum(Y){p(X|Y )p(Y ).}

We can view the denominator in Bayes’ theorem as being the normalization constant required to ensure that the sum of the conditional probability on the left-hand side of
(1.12) over all values of Y equals one.


 we note that if the joint distribution of two variables factorizes into the product of the marginals, so that p(X, Y ) = p(X)p(Y ), then X and Y are said to be independent


From the product rule, we see that p(Y |X) = p(Y ), and so the conditional distribution of Y given X is indeed independent of the value of X


Expectations and covariances:

The average value of some function f(x) under a
probability distribution p(x)(ie for each value of f(x) there will be a value which gives its probability of hapening) is called the expectation of f(x) and will be denoted by E[f]. For a discrete distribution, it is given by


E[F] = Sum(x){P(x)*f(x)}
so that the average is weighted by the relative probabilities of the different values
of x

if we are given a finite number N of points drawn from the probability distribution or probability density, then the expectation can be approximated as a finite sum over these points


E[f] = (1/N)SUM(n=1 to N){f(Xn)}

Sometimes we will be considering expectations of functions of several variables, in which case we can use a subscript to indicate which variable is being averaged over, so that for instance

Ex[f(x, y)]

We can also consider a conditional expectation with respect to a conditional distribution, so that

Ex[f|y] = SUM(x){p(x|y)f(x)}


For two random variables x and y, the covariance is defined by

cov[x, y] = Ex,y [{x − E[x]} {y − E[y]}]
	  = Ex,y[xy] − E[x]E[y]

which expresses the extent to which x and y vary together. If x and y are independent,then their covariance vanishes.

In the case of two vectors of random variables x and y, the covariance is a matrix

cov[x, y] = Ex,y {x − E[x]}{yT − E[yT]}]
	  = Ex,y[xyT] − E[x]E[yT]

If we consider the covariance of the components of a vector x with each other, then we use a slightly simpler notation 

cov[x] ≡ cov[x, x].

 Bayesian probabilities:

We capture our assumptions about w, before observing the data, in the form of a prior probability distribution p(w). The effect of the observed data D = {t1,...,tN} is expressed through the conditional probability p(D|w), 

p(w|D) = p(D|w)p(w)/p(D)


The quantity p(D|w) on the right-hand side of Bayes’ theorem is evaluated for the observed data set D and can be viewed as a function of the parameter vector
w, in which case it is called the likelihood function. It expresses how probable the observed data set is for different settings of the parameter vector w. Note that the
likelihood is not a probability distribution over w, and its integral with respect to w does not (necessarily) equal one. Given this definition of likelihood, we can state Bayes’ theorem in words

posterior ∝ likelihood × prior........1.43

where all of these quantities are viewed as functions of w. The denominator in (1.43) is the normalization constant, which ensures that the posterior distribution
on the left-hand side is a valid probability density and integrates to one.

In both the Bayesian and frequentist paradigms, the likelihood function p(D|w) plays a central role. However, the manner in which it is used is fundamentally different in the two approaches. In a frequentist setting, w is considered to be a fixed parameter, whose value is determined by some form of ‘estimator’, and error bars


on this estimate are obtained by considering the distribution of possible data sets D. By contrast, from the Bayesian viewpoint there is only a single data set D (namely the one that is actually observed), and the uncertainty in the parameters is expressed
through a probability distribution over w


A widely used frequentist estimator is maximum likelihood, in which w is set to the value that maximizes the likelihood function p(D|w). This corresponds to
choosing the value of w for which the probability of the observed data set is maximized. In the machine learning literature, the negative log of the likelihood function
is called an error function. Because the negative logarithm is a monotonically decreasing function, maximizing the likelihood is equivalent to minimizing the error.


One approach to determining frequentist error bars is the bootstrap, in which multiple data sets are created as follows. Suppose our original data set consists of N data points X = {x1, . . . , xN}. We can create a new
data set XB by drawing N points at random from X, with replacement, so that some points in X may be replicated in XB, whereas other points in X may be absent from XB. This process can be repeated L times to generate L data sets each of size N and each obtained by sampling from the original data set X. The statistical accuracy of
parameter estimates can then be evaluated by looking at the variability of predictions between the different bootstrap data sets.

One advantage of the Bayesian viewpoint is that the inclusion of prior knowledge arises naturally. Suppose, for instance, that a fair-looking coin is tossed three
times and lands heads each time. A classical maximum likelihood estimate of the probability of landing heads would give 1, implying that all future tosses will land
heads! By contrast, a Bayesian approach with any reasonable prior will lead to a much less extreme conclusion.


There has been much controversy and debate associated with the relative merits of the frequentist and Bayesian paradigms, which have not been helped by the
fact that there is no unique frequentist, or even Bayesian, viewpoint. For instance, one common criticism of the Bayesian approach is that the prior distribution is often selected on the basis of mathematical convenience rather than as a reflection of any prior beliefs. Even the subjective nature of the conclusions through their dependence on the choice of prior is seen by some as a source of difficulty. Reducing the dependence on the prior is one motivation for so-called noninformative priors. However, these lead to difficulties when comparing different models, and indeed Bayesian methods based on poor choices of prior can give poor results with high
confidence. Frequentist evaluation methods offer some protection from such problems, and techniques such as cross-validation remain useful in areas such as model
comparison.






















*The Gaussian distribution:
For the case of a single real-valued variable x, the Gaussian distribution is defined by

		  1		      -1*(x - µ)^2
N(x|µ, σ^2) =  ----------------  exp{----          }..............5.0
		(2*pi*σ^2)^0.5        2*σ^2


which is governed by two parameters: µ, called the mean, and σ2, called the variance. The square root of the variance, given by σ, is called the standard deviation,
and the reciprocal of the variance, written as β = 1/σ2, is called the precision.


from 5.0 it is clear that N(x|µ, σ^2) > 0

Also it is straightforward to show that the Gaussian is normalized

IMAGE: checkout image GD1,GD2 in images

Now suppose that we have a data set of observations x = (x1, . . . , xN)T, representing N observations of the scalar variable x.We shall suppose that the observations are drawn independently from a Gaussian distribution whose mean µ and variance σ2 are unknown, and we would like to determine these parameters from the data set. Data points that are drawn independently from the same distribution are said to be independent and identically distributed, which is often abbreviated to i.i.d. We have seen that the joint probability of two independent events is given by the product of the marginal probabilities for each event separately. Because our data set x is i.i.d., we can therefore write the probability of the data set, given µ and σ2, in the form

p(x|µ, σ2) = product(from n=1 to n=N){N( Xn|µ, σ^2)}..........5.1

When viewed as a function of µ and σ^2, this is the likelihood function for the Gaussian and is interpreted as 

IMAGE GD3 in images folder

One common criterion for determining the parameters in a probability distribution using an observed data set is to find the parameter values that maximize the likelihood function

we shall determine values for the unknown parameters µ and σ2 in the Gaussian by maximizing the likelihood function in 5.1

 In practice, it is more convenient to maximize the log of the likelihood function. Because the logarithm is a monotonically increasing function of its argument, maximization of the log of a function is equivalent to maximization of the function itself. Taking the log not only simplifies the subsequent mathematical analysis, but it also helps numerically because the product of a large number of small probabilities can easily underflow the numerical precision of the computer, and this is resolved by computing instead the sum of the log probabilities. 

the equation see IMAGE GD4


the significant limitations of the maximum likelihood approach is that the maximum likelihood approach systematically underestimates the variance
of the distribution. This is an example of a phenomenon called bias and is related to the problem of over-fitting encountered in the context of polynomial curve fitting.


IMAGE GD5,GD6

Note that the bias of the maximum likelihood solution becomes less significant as the number N of data points increases, and in the limit N → ∞ the maximum likelihood solution for the variance equals the true variance of the distribution that generated the data. In practice, for anything other than small N, this bias will not prove to be a serious problem. However, throughout this book we shall be interested in more complex models with many parameters, for which the bias problems associated with maximum likelihood will be much more severe. In fact, as we shall see, the issue of bias in maximum likelihood lies at the root of the over-fitting problem that we encountered earlier in the context of polynomial curve fitting.



Curve fitting using basiean approach:(Curve fitting revisited)


We have seen how the problem of polynomial curve fitting can be expressed in terms of error minimization. Here we return to the curve fitting example and view it
from a probabilistic perspective, thereby gaining some insights into error functions and regularization, as well as taking us towards a full Bayesian treatment



The goal in the curve fitting problem is to be able to make predictions for the target variable t given some new value of the input variable x on the basis of a set of
training data comprising N input values x = (x1, . . . , xN)T and their corresponding target values t = (t1, . . . , tN)T. We can express our uncertainty over the value of the target variable using a probability distribution. For this purpose, we shall assume that, given the value of x, the corresponding value of t has a Gaussian distribution with a mean equal to the value y(x, w) of the polynomial curve given by (1.1). Thus we have

p(t|x, w, β) = N(t|y(x, w), β**-1)....... (1.60),
where
y(x, w) = w0 + w1*X + w2*X^2 + . . . + wM*X^M
y(x, w) is the mean
β**-1 = variance = σ^2


see IMAGE GD7

We now use the training data {x, t} to determine the values of the unknown parameters w and β by maximum likelihood. If the data are assumed to be drawn
independently from the distribution (1.60), then the likelihood function is given by

p(t|x, w,β) = product(from n=1 to N){N(tn|y(xn, w),β**−1}.


As we did in the case of the simple Gaussian distribution earlier, it is convenient to maximize the logarithm of the likelihood function. Substituting for the form of the
Gaussian distribution, given by (1.46), we obtain the log likelihood function in the form

ln{p(t|x,w,β)} = -(β/2)* SUM(from n=1 to N){y(xn,w)-tn}^2 + N/2 *ln(β) - N/2 * ln(2*pi).........1.62

 
Consider first the determination of the maximum likelihood solution for the polynomial coefficients, which will be denoted by wML. These are determined by maximizing (1.62) with respect to w. For this purpose, we can omit the last two terms on the right-hand side of (1.62) because they do not depend on w. Also, we note that scaling the log likelihood by a positive constant coefficient does not alter the location of the maximum with respect to w, and so we can replace the coefficient β/2 with 1/2. Finally, instead of maximizing the log likelihood, we can equivalently minimize the negative log likelihood. We therefore see that maximizing likelihood is equivalent, so far as determining w is concerned, to minimizing the sum-of-squares error function defined by 

E(w) = 1/2 * SUM(from n=1 to N){y(xn,w) - tn}^2.....1.2

E(w) = error function or cost function, when this is minimized we get

Thus the sum-of-squares error function has arisen as
a consequence of maximizing likelihood under the assumption of a Gaussian noise distribution.


We can also use maximum likelihood to determine the precision parameter β of the Gaussian conditional distribution. Maximizing (1.62) with respect to β gives

1/βML = 1/N * SUM(from n=1 to N){y(xn, wML) − tn}^2


Having determined the parameters w and β, we can now make predictions for new values of x. Because we now have a probabilistic model, these are expressed in terms of the predictive distribution that gives the probability distribution over t, rather than simply a point estimate, and is obtained by substituting the maximum likelihood parameters(wML,βML) into (1.60) to give

p(t|x, wML, βML) = N(t|y(x, wML), βML**−1).....1.64


Now let us take a step towards a more Bayesian approach and introduce a prior distribution over the polynomial coefficients w. For simplicity, let us consider a
Gaussian distribution of the form

p(w|α) = N(w|0,(α^-1)I) = (α/2*pi)^((M+1)/2) * exp{(−α/2) wTw}...........1.65

where α is alpha, I is the Identity matrix, wT is the transpose of w

where α is the precision of the distribution, and M+1 is the total number of elements in the vector w for an M th order polynomial. Variables such as α, which control
the distribution of model parameters(w,β), are called hyperparameters. Using Bayes’theorem, the posterior distribution for w is proportional to the product of the prior distribution and the likelihood function


p(w|x, t, α, β) ∝ p(t|x, w, β)*p(w|α)....... (1.66)


We can now determine w by finding the most probable value of w given the data, in other words by maximizing the posterior distribution. This technique is called
maximum posterior, or simply MAP. Taking the negative logarithm of (1.66) and combining with (1.62) and (1.65), we find that the maximum of the posterior is given by the minimum of

(β/2) * SUM(from n=1 to N){y(xn,w) - tn}^2 + (α/2) * wTw

Thus we see that maximizing the posterior distribution is equivalent to minimizing the regularized sum-of-squares error function encountered earlier in the form (1.4),
with a regularization parameter given by λ = α/β.


BAYESIAN CURVE FITTING:

Although we have included a prior distribution p(w|α), we are so far still making a point estimate of w and so this does not yet amount to a Bayesian treatment. In
a fully Bayesian approach, we should consistently apply the sum and product rules of probability, which requires, as we shall see shortly, that we integrate over all values of w. Such marginalizations lie at the heart of Bayesian methods for pattern
recognition

In the curve fitting problem, we are given the training data x and t, along with a new test point x, and our goal is to predict the value of t. We therefore wish
to evaluate the predictive distribution p(t|x, x, t). Here we shall assume that the parameters α and β are fixed and known in advance (in later chapters we shall discuss
how such parameters can be inferred from data in a Bayesian setting).

A Bayesian treatment simply corresponds to a consistent application of the sum and product rules of probability, which allow the predictive distribution to be written
in the form


p(t|x, X, t) = INTEGRAL{p(t|x, w)p(w|x, t)dw}.......(1.68)

x- single point
X - vecor of x's

Here p(t|x, w) is given by (1.60), and we have omitted the dependence on α and β to simplify the notation. Here p(w|x, t) is the posterior distribution over parameters, and can be found by normalizing the right-hand side of (1.66). We shall see in Section 3.3 that, for problems such as the curve-fitting example, this posterior distribution is a Gaussian and can be evaluated analytically. Similarly, the integration in (1.68) can also be performed analytically with the result that the predictive distribution is given by a Gaussian of the form


p(t|x, X, t) = N(t|m(x), s^2(x))........... (1.69)

where,s^2(x) = s square of x
where,mean and the variance are given by

m(x) = β * φ(x)T * S * SUM(from n=1 to N){φ(xn) * tn}...1.70

s^2(x) = β**-1 + φ(x)T S φ(x)....1.71

here S**-1 = αI + β SUM(from n=1 to N){ φ(xn) φ(x)T }.......1.72

where I is the unit matrix, and we have defined the vector φ(x) with elements
φi(x) = xi for i = 0, . . . , M


We see that the variance, as well as the mean, of the predictive distribution in (1.69) is dependent on x. The first term in (1.71) represents the uncertainty in the
predicted value of t due to the noise on the target variables and was expressed already in the maximum likelihood predictive distribution (1.64) through βML −1. However, the second term arises from the uncertainty in the parameters w and is a consequence of the Bayesian treatment. The predictive distribution for the synthetic sinusoidal regression problem is illustrated in Figure 1.17.


SEE IMAGE GD8


MODEL SELECTION:
In our example of polynomial curve fitting using least squares, we saw that there was an optimal order of polynomial that gave the best generalization. The order of the polynomial controls the number of free parameters in the model and thereby governs the model complexity. With regularized least squares, the regularization coefficient
λ also controls the effective complexity of the model, whereas for more complex models, such as mixture distributions or neural networks there may be multiple parameters governing complexity. In a practical application, we need to determine the values of such parameters, and the principal objective in doing so is usually to achieve the best predictive performance on new data. Furthermore, as well as finding the appropriate values for complexity parameters within a given model, we may wish to consider a range of different types of model in order to find the best one for
our particular application.


We have already seen that, in the maximum likelihood approach, the performance on the training set is not a good indicator of predictive performance on unseen data due to the problem of over-fitting. If data is plentiful, then one approach is simply to use some of the available data to train a range of models, or a given model
with a range of values for its complexity parameters, and then to compare them on independent data, sometimes called a validation set, and select the one having the
best predictive performance. If the model design is iterated many times using a limited size data set, then some over-fitting to the validation data can occur and so it may be necessary to keep aside a third test set on which the performance of the selected
model is finally evaluated.

In many applications, however, the supply of data for training and testing will be limited, and in order to build good models, we wish to use as much of the available
data as possible for training. However, if the validation set is small, it will give a relatively noisy estimate of predictive performance. One solution to this dilemma is
to use cross-validation, which is illustrated in Figure 1.18. This allows a proportion (S −1)/S of the available data to be used for training while making use of all of the
data to assess performance. When data is particularly scarce, it may be appropriate to consider the case S = N, where N is the total number of data points, which gives
the leave-one-out technique.

SEE IMAGE GD9

One major drawback of cross-validation is that the number of training runs that must be performed is increased by a factor of S, and this can prove problematic for
models in which the training is itself computationally expensive. A further problem with techniques such as cross-validation that use separate data to assess performance
is that we might have multiple complexity parameters for a single model (for instance, there might be several regularization parameters). Exploring combinations
of settings for such parameters could, in the worst case, require a number of training runs that is exponential in the number of parameters. Clearly, we need a better approach. Ideally, this should rely only on the training data and should allow multiple hyperparameters and model types to be compared in a single training run. We therefore need to find a measure of performance which depends only on the training data and which does not suffer from bias due to over-fitting.


The curse of Dimensionality:

As the number of inputs or features increases it poses some serious challenges and is an important factor influencing the design of
pattern recognition techniques.

D = number of inputs

As D increases, so the number of independent coefficients (not all of the coefficients are independent due to interchange symmetries amongst the x variables) grows proportionally to D^3. I

In practice, to capture complex dependencies in the data, we may
need to use a higher-order polynomial. For a polynomial of order M, the growth in the number of coefficients is like D^M . Although this is now a power law growth, rather than an exponential growth, it still points to the method becoming rapidly unwieldy and of limited practical utility.


Although the curse of dimensionality certainly raises important issues for pattern recognition applications, it does not prevent us from finding effective techniques
applicable to high-dimensional spaces. The reasons for this are twofold. First, real data will often be confined to a region of the space having lower effective dimensionality, and in particular the directions over which important variations in the target
variables occur may be so confined. Second, real data will typically exhibit some smoothness properties (at least locally) so that for the most part small changes in the input variables will produce small changes in the target variables, and so we can exploit local interpolation-like techniques to allow us to make predictions of the target variables for new values of the input variables.




*Decision theory:

decision theory, when combined with probability theory,allows us to make optimal decisions in situations involving uncertainty such as those
encountered in pattern recognition.

Decision theory is the subject of decision theory to tell us how to make optimal decisions given the appropriate probabilities. We shall see that the decision stage is generally very simple, even trivial, once we have solved the inference problem.

Before giving a more detailed analysis, let us first consider informally how we might expect probabilities to play a role in making decisions. When we obtain the X-ray image x for a new patient, our goal is to decide which of the two classes to assign to the image. We are interested in the probabilities of the two classes given the image, which are given by p(Ck|x). Using Bayes’ theorem, these probabilities
can be expressed in the form

p(Ck|x) = p(x|Ck)p(Ck)/p(x)........ (1.77)

Note that any of the quantities appearing in Bayes’ theorem can be obtained from the joint distribution p(x, Ck) by either marginalizing or conditioning with respect to the appropriate variables. We can now interpret p(Ck) as the prior probability for the class Ck, and p(Ck|x) as the corresponding posterior probability. Thus p(C1) represents the probability that a person has cancer, before we take the X-ray measurement. Similarly, p(C1|x) is the corresponding probability, revised using Bayes’ theorem in light of the information contained in the X-ray. If our aim is to minimize the chance of assigning x to the wrong class, then intuitively we would choose the class having
the higher posterior probability. We now show that this intuition is correct, and we also discuss more general criteria for making decisions.

	

Minimizing the misclassification rate:

Suppose that our goal is simply to make as few misclassifications as possible.We need a rule that assigns each value of x to one of the available classes. Such a rule will divide the input space into regions Rk called decision regions, one for each class, such that all points in Rk are assigned to class Ck. The boundaries between decision regions are called decision boundaries or decision surfaces. Note that each
decision region need not be contiguous but could comprise some number of disjoint regions. We shall encounter examples of decision boundaries and decision regions in later chapters. In order to find the optimal decision rule, consider first of all the case of two classes, as in the cancer problem for instance. A mistake occurs when an input vector belonging to class C1 is assigned to class C2 or vice versa. The probability of this occurring is given by


p(mistake) = p(x ∈ R1, C2) + p(x ∈ R2, C1)
	   = INTEGRAL(from R1) {p(x, C2)dx} + INTEGRAL(from R2){ p(x, C1)dx}

We are free to choose the decision rule that assigns each point x to one of the two classes. Clearly to minimize p(mistake) we should arrange that each x is assigned to whichever class has the smaller value of the integrand in (1.78). Thus, if p(x, C1) > p(x, C2) for a given value of x, then we should assign that x to class C1. From the
product rule of probability we have p(x, Ck) = p(Ck|x)p(x). Because the factor p(x) is common to both terms, we can restate this result as saying that the minimum


probability of making a mistake is obtained if each value of x is assigned to the class for which the posterior probability p(Ck|x) is largest. This result is illustrated for two classes, and a single input variable x, in Figure 1.24.

IMAGE GD17


For the more general case of K classes, it is slightly easier to maximize the probability of being correct, which is given by

p(correct) = SUM(from k= to K){p(x ∈ Rk, Ck)}

p(correct) = SUM(from k= to K)INTEGRAL(Rk){p(x ∈ Rk, Ck)}


which is maximized when the regions Rk are chosen such that each x is assigned to the class for which p(x, Ck) is largest. Again, using the product rule p(x, Ck) = p(Ck|x)p(x), and noting that the factor of p(x) is common to all terms, we see that each x should be assigned to the class having the largest posterior probability p(Ck|x).

IMAGE GD18
















=================================================================
BAYESIAN STATISTICS:(course: Coursera,Introduction to Bayesian statistics)

Bayes rule:
	  P(A&B) * P(A)
P(A|B) = --------------------
	      P(B)



In baysian stats we have something called Confusion matrix,
where 

True positive is called sensitivity
True negative is called specitivity 

We can update the priors using the calculated posteriors in the previous step.

























=================
SOURCE :mathematicalmonk's channel YOuTUBE

Types of Unsupervised learning:

* Clustering
* Density estimation
* Dimensionality reduction

variations on Machine learning techniques:
========================================
Source : Wiki 
1.Semi-supervised learningk:Semi-supervised learning is a class of supervised learning tasks and techniques that also make use of unlabeled data for training – typically a small amount of labeled data with a large amount of unlabeled data. Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy. The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render a fully labeled training set infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value. Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning.


As in the supervised learning framework, we are given a set of l independently identically distributed examples x1,.....,xl e X with corresponding labels y1,....yl e Y. Additionally,we are given u unlabeled examples xl+1,....xl+u e X.Semi-supervised learning attempts to make use of this combined information to surpass the classification performance that could be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning.Semi-supervised learning may refer to either transductive learning or inductive learning. The goal of transductive learning is to infer the correct labels for the given unlabeled data xl+1,....xl+u only. the goal od inductive learning is to refer the correct mapping from X to Y

Intuitively, we can think of the learning problem as an exam and labeled data as the few example problems that the teacher solved in class. The teacher also provides a set of unsolved problems. In the transductive setting, these unsolved problems are a take-home exam and you want to do well on them in particular. In the inductive setting, these are practice problems of the sort you will encounter on the in-class exam.

It is unnecessary (and, according to Vapnik's principle, imprudent) to perform transductive learning by way of inferring a classification rule over the entire input space; however, in practice, algorithms formally designed for transduction or induction are often used interchangeably.

2.Decision theory:



3.Reinforcement learning:
here we want to maximize lifetime rewards


Generative vs Discriminate models:
================================= 










