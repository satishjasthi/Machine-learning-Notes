Source : Machine learning in Action txt book
Knowledge representation of an algo is nothing but the knowledge that the machine has gained on data when it was trained on training data.

NumPy matrix vs. array: 
In NumPy there are two different data types for dealing with rows and columns of numbers. Be careful of this because they look similar, but simple mathematical operations such as multiply on the two data types can have different meanings. The matrix data type behaves more like matrices in MATLAB.™

Ex: n = np.random.rand(5,5) # creates a matrix with datatype of arrray
whereas 
n = np.mat(np.random.rand(5,5)) # creates a matrix with datatype  matrix

to get the inverse of matrix say A in pyhton we use
Inverse_A = A.I 

to get an identity matrix:
 I = np.eye(O) # O: is the order of matrix

to gets zeros:
a = (5,5)
 Z = np.zeros(a) # creates a 5X5 zero matrix

Classifying with k-Nearest Neighbors:
---------------------------------------------------------------------
k-Nearest Neighbors
Pros: High accuracy, insensitive to outliers, no assumptions about data
Cons: Computationally expensive, requires a lot of memory
Works with: Numeric values, nominal values(means 1,0)
----------------------------------------------------------------------

General approach to kNN:
1. Collect: Any method.
2. Prepare: Numeric values are needed for a distance calculation. A structured data format is best.
3. Analyze: Any method.
4. Train: Does not apply to the kNN algorithm.
5. Test: Calculate the error rate.
6. Use: This application needs to get some input data and output structured numeric values. Next, the application runs the kNN algorithm on this input data and determines which class the input data should belong to. The application then takes some action on the calculated class.

Euclidian distance : is the simple distance formula to calculate the distance between any two points in a plane,

d  = sqrt((X2-X1)**2 + (Y2-Y1)**2)

but the good thing is that we can use the same formula to calculate the distance b/w 2 points in n dimensional space

consider 2 points A = (x1,y1,z1,r1,k1.......,n1) ,B = (x2,y2,z2,r2,k2.......,n2)

d  = sqrt((X2-X1)**2 + (Y2-Y1)**2 + (Z2-Z1)**2 + .... + (n2-n1)**2)


error rate: Number of inputs algorithm wrongly classified given input , divide by total number of inputs givent to the algo.

Parsing:The actual definition of "parse" in Wiktionary is "To split a file or other input into pieces of data that can be easily stored or manipulated." 


Data preprocessing: whenever the values of the features vary over a wide range and their range differ significantly from each other, in such cases is appropriate to scale the features, which helps in providing equal importance to all features.YOu can use min-max normaliser from sklearn for this,whose formula is

new_value = (old_value - min)/(max-min)

whenever you need to calculate the min or max value of a column in dataset you can use

datasetname.min(0)
datasetname.mix(0)

it returns min values of all columns as a row vector ,where each element of the vector is the min value each column

similiary to get the min/max value of a row

datasetname.min(1)

The function np.tile(value,repetation) creates an array of repeated values

suppose 
>>> np.tile(5,2)
array([5, 5])

>>> np.tile(5,(2,2))
array([[5, 5],
       [5, 5]])

>>> a = np.matrix([[1,2,3],[5,8,9]])
>>> a
matrix([[1, 2, 3],
        [5, 8, 9]])
>>> np.tile(a,2)
matrix([[1, 2, 3, 1, 2, 3],
        [5, 8, 9, 5, 8, 9]])
>>> np.tile(a,(2,2))
matrix([[1, 2, 3, 1, 2, 3],
        [5, 8, 9, 5, 8, 9],
        [1, 2, 3, 1, 2, 3],
        [5, 8, 9, 5, 8, 9]])


while using numpy '/' operator means element wise division but to carry out matrix division you must use 
linalg.solve(matA,matB)

Drawbacks of KNN:

kNN is an example of instance-based learning, where you need to have instances of data close at hand to perform the machine learning algorithm. The algorithm has to carry around the full dataset; for large datasets, this implies a large amount of storage. In addition, you need to calculate the distance measurement for every piece of data in the database, and this can be cumbersome.

An additional drawback is that kNN doesn’t give you any idea of the underlying structure of the data; you have no idea what an “average” or “exemplar” instance from each class looks like.
==============================================================
 Source:https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/


KNN is a non-parametric algorithm, which means that it does not make any assumptions on the underlying data distribution.Which is pretty usefull since in case of most of the real world data we don't know the underlying assumptions within data,in such cases KNN can help a lot

It is also a lazy algorithm. What this means is that it does not use the training data points to do any generalization. In other words, there is no explicit training phase or it is very minimal. This means the training phase is pretty fast . Lack of generalization means that KNN keeps all the training data. More exactly, all the training data is needed during the testing phase. (Well this is an exaggeration, but not far from truth). This is in contrast to other techniques like SVM where you can discard all non support vectors without any problem.  Most of the lazy algorithms – especially KNN – makes decision based on the entire training data set (in the best case a subset of them).

Each of the training data consists of a set of vectors and class label associated with each vector. In the simplest case , it will be either + or – (for positive or negative classes). But KNN , can work equally well with arbitrary number of classes.

We are also given a single number "k" . This number decides how many neighbors (where neighbors is defined based on the distance metric) influence the classification. This is usually a odd number if the number of classes is 2. If k=1 , then the algorithm is simply called the nearest neighbor algorithm.


Case 1 : k = 1 or Nearest Neighbor Rule
This is the simplest scenario. Let x be the point to be labeled . Find the point closest to x . Let it be y. Now nearest neighbor rule asks to assign the label of y to x. This seems too simplistic and some times even counter intuitive. If you feel that this procedure will result a huge error , you are right – but there is a catch. This reasoning holds only when the number of data points is not very large.

If the number of data points is very large, then there is a very high chance that label of x and y are same. An example might help – Lets say you have a (potentially) biased coin. You toss it for 1 million time and you have got head 900,000 times. Then most likely your next call will be head. We can use a similar argument here.

Let me try an informal argument here -  Assume all points are in a D dimensional plane . The number of points is reasonably large. This means that the density of the plane at any point is fairly high. In other words , within any subspace there is adequate number of points. Consider a point x in the subspace which also has a lot of neighbors. Now let y be the nearest neighbor. If x and y are sufficiently close, then we can assume that probability that x and y belong to same class is fairly same 


Case 2 : k = K or k-Nearest Neighbor Rule
This is a straightforward extension of 1NN. Basically what we do is that we try to find the k nearest neighbor and do a majority voting. Typically k is odd when the number of classes is 2. Lets say k = 5 and there are 3 instances of C1 and 2 instances of C2. In this case , KNN says that new point has to labeled as C1 as it forms the majority. We follow a similar argument when there are multiple classes.

One of the straight forward extension is not to give 1 vote to all the neighbors. A very common thing to do is weighted kNN where each point has a weight which is typically calculated using its distance. For eg under inverse distance weighting, each point has a weight equal to the inverse of its distance to the point to be classified. This means that neighboring points have a higher vote than the farther points.

It is quite obvious that the accuracy *might* increase when you increase k but the computation cost also increases.

Some Basic Observations
1. If we assume that the points are d-dimensional, then the straight forward implementation of finding k Nearest Neighbor takes O(dn) time. 

6. Choice of k is very critical – A small value of k means that noise will have a higher influence on the result. A large value make it computationally expensive and kinda defeats the basic philosophy behind KNN (that points that are near might have similar densities or classes ) .A simple approach to select k is set
k = sqrt(n)


Applications of KNN:

KNN is a versatile algorithm and is used in a huge number of fields. Let us take a look at few uncommon and non trivial applications.

1. Nearest Neighbor based Content Retrieval 
This is one the fascinating applications of KNN – Basically we can use it in Computer Vision for many cases – You can consider handwriting detection as a rudimentary nearest neighbor problem. The problem becomes more fascinating if the content is a video – given a video find the video closest to the query from the database – Although this looks abstract, it has lot of practical applications – Eg : Consider ASL (American Sign Language)  . Here the communication is done using hand gestures.

So lets say if we want to prepare a dictionary for ASL so that user can query it doing a gesture. Now the problem reduces to find the (possibly k) closest gesture(s) stored in the database and show to user. In its heart it is nothing but a KNN problem. One of the professors from my dept , Vassilis Athitsos , does research in this interesting topic – See Nearest Neighbor Retrieval and Classification for more details.

2. Gene Expression 
This is another cool area where many a time, KNN performs better than other state of the art techniques . In fact a combination of KNN-SVM is one of the most popular techniques there. This is a huge topic on its own and hence I will refrain from talking much more about it.

3. Protein-Protein interaction and 3D structure prediction 
Graph based KNN is used in protein interaction prediction. Similarly KNN is used in structure prediction.


===================================================================
Source:http://machinelearningmastery.com/k-nearest-neighbors-for-machine-learning/

In this post you will discover the k-Nearest Neighbors (KNN) algorithm for classification and regression. After reading this post you will know.

The model representation used by KNN.
How a model is learned using KNN (hint, it’s not).
How to make predictions using KNN
The many names for KNN including how different fields refer to it.
How to prepare your data to get the most from KNN.
Where to look to learn more about the KNN algorithm.
This post was written for developers and assumes no background in statistics or mathematics. The focus is on how the algorithm works and how to use it for predictive modeling problems. If you have any questions, leave a comment and I will do my best to answer.

Let’s get started.


KNN Model Representation

The model representation for KNN is the entire training dataset.

It is as simple as that.

KNN has no model other than storing the entire dataset, so there is no learning required.

Efficient implementations can store the data using complex data structures like k-d trees to make look-up and matching of new patterns during prediction efficient.

Because the entire training dataset is stored, you may want to think carefully about the consistency of your training data. It might be a good idea to curate it, update it often as new data becomes available and remove erroneous and outlier data.

Making Predictions with KNN

KNN makes predictions using the training dataset directly.

Predictions are made for a new instance (x) by searching through the entire training set for the K most similar instances (the neighbors) and summarizing the output variable for those K instances. For regression this might be the mean output variable, in classification this might be the mode (or most common) class value.

To determine which of the K instances in the training dataset are most similar to a new input a distance measure is used. For real-valued input variables, the most popular distance measure is Euclidean distance.

Euclidean distance is calculated as the square root of the sum of the squared differences between a new point (x) and an existing point (xi) across all input attributes j.

EuclideanDistance(x, xi) = sqrt( sum( (xj – xij)^2 ) )

Other popular distance measures include:

Hamming Distance: Calculate the distance between binary vectors (more).
Manhattan Distance: Calculate the distance between real vectors using the sum of their absolute difference. Also called City Block Distance (more).
Minkowski Distance: Generalization of Euclidean and Manhattan distance (more).
There are many other distance measures that can be used, such as Tanimoto, Jaccard, Mahalanobis and cosine distance. You can choose the best distance metric based on the properties of your data. If you are unsure, you can experiment with different distance metrics and different values of K together and see which mix results in the most accurate models.

Euclidean is a good distance measure to use if the input variables are similar in type (e.g. all measured widths and heights). Manhattan distance is a good measure to use if the input variables are not similar in type (such as age, gender, height, etc.).

The value for K can be found by algorithm tuning. It is a good idea to try many different values for K (e.g. values from 1 to 21) and see what works best for your problem.

The computational complexity of KNN increases with the size of the training dataset. For very large training sets, KNN can be made stochastic by taking a sample from the training dataset from which to calculate the K-most similar instances.

KNN has been around for a long time and has been very well studied. As such, different disciplines have different names for it, for example:

Instance-Based Learning: The raw training instances are used to make predictions. As such KNN is often referred to as instance-based learning or a case-based learning (where each training instance is a case from the problem domain).
Lazy Learning: No learning of the model is required and all of the work happens at the time a prediction is requested. As such, KNN is often referred to as a lazy learning algorithm.
Non-Parametric: KNN makes no assumptions about the functional form of the problem being solved. As such KNN is referred to as a non-parametric machine learning algorithm.
KNN can be used for regression and classification problems.

KNN for Regression

When KNN is used for regression problems the prediction is based on the mean or the median of the K-most similar instances.

KNN for Classification

When KNN is used for classification, the output can be calculated as the class with the highest frequency from the K-most similar instances. Each instance in essence votes for their class and the class with the most votes is taken as the prediction.

Class probabilities can be calculated as the normalized frequency of samples that belong to each class in the set of K most similar instances for a new data instance. For example, in a binary classification problem (class is 0 or 1):

p(class=0) = count(class=0) / (count(class=0)+count(class=1))

If you are using K and you have an even number of classes (e.g. 2) it is a good idea to choose a K value with an odd number to avoid a tie. And the inverse, use an even number for K when you have an odd number of classes.

Ties can be broken consistently by expanding K by 1 and looking at the class of the next most similar instance in the training dataset.

Curse of Dimensionality

KNN works well with a small number of input variables (p), but struggles when the number of inputs is very large.

Each input variable can be considered a dimension of a p-dimensional input space. For example, if you had two input variables x1 and x2, the input space would be 2-dimensional.

As the number of dimensions increases the volume of the input space increases at an exponential rate.

In high dimensions, points that may be similar may have very large distances. All points will be far away from each other and our intuition for distances in simple 2 and 3-dimensional spaces breaks down. This might feel unintuitive at first, but this general problem is called the “Curse of Dimensionality“.

Best Prepare Data for KNN

Rescale Data: KNN performs much better if all of the data has the same scale. Normalizing your data to the range [0, 1] is a good idea. It may also be a good idea to standardize your data if it has a Gaussian distribution.
Address Missing Data: Missing data will mean that the distance between samples can not be calculated. These samples could be excluded or the missing values could be imputed.
Lower Dimensionality: KNN is suited for lower dimensional data. You can try it on high dimensional data (hundreds or thousands of input variables) but be aware that it may not perform as well as other techniques. KNN can benefit from feature selection that reduces the dimensionality of the input feature space.
Further Reading

If you are interested in implementing KNN from scratch in Python, checkout the post:

Tutorial To Implement k-Nearest Neighbors in Python From Scratch
Below are some good machine learning texts that cover the KNN algorithm from a predictive modeling perspective.

Applied Predictive Modeling, Chapter 7 for regression, Chapter 13 for classification.
Data Mining: Practical Machine Learning Tools and Techniques, page 76 and 128
Doing Data Science: Straight Talk from the Frontline, page 71
Machine Learning, Chapter 8
Also checkout K-Nearest Neighbors on Wikipedia.

Summary

In this post you discovered the KNN machine learning algorithm. You learned that:

KNN stores the entire training dataset which it uses as its representation.
KNN does not learn any model.
KNN makes predictions just-in-time by calculating the similarity between an input sample and each training instance.
There are many distance measures to choose from to match the structure of your input data.
That it is a good idea to rescale your data, such as using normalization, when using KNN.
If you have any questions about this post or the KNN algorithm ask in the comments and I will do my best to answer.











============================================================

DECISION TREES:
================

Decision trees
Pros: Computationally cheap to use, easy for humans to understand learned results, missing values OK, can deal with irrelevant features
Cons: Prone to overfitting
Works with: Numeric values, nominal values

Information theory is the key element that helps Decision trees to create branches and split data based several conditions.

To build a decision tree, you need to make a first decision on the dataset to dictate which feature is used to split the data. To determine this, you try every feature and measure which split will give you the best results. After that, you’ll split the dataset into subsets. The subsets will then traverse down the branches of the first decision node. If the data on the branches is the same class, then you’ve properly classified it and don’t need to continue splitting it. If the data isn’t the same, then you need to repeat the splitting
process on this subset. The decision on how to split this subset is done the same way as the original dataset, and you repeat this process until you’ve classified all the data

Pseudo-code for a function called createBranch() would look like this:

Check if every item in the dataset is in the same class:
	If so return the class label
	Else
		find the best feature to split the data
		split the dataset
		create a branch node
		for each split
			call createBranch and add the result to the branch node
		return branch node

Please note the recursive nature of createBranch. It calls itself in the second-to-last line. 

General approach to decision trees
1. Collect: Any method.
2. Prepare: This tree-building algorithm works only on nominal values, so any continuous values will need to be quantized.
3. Analyze: Any method. You should visually inspect the tree after it is built.
4. Train: Construct a tree data structure.
5. Test: Calculate the error rate with the learned tree.
6. Use: This can be used in any supervised learning task. Often, trees are used to better understand the data.


Some decision trees make a binary split of the data, but we won’t do this. If we split on an attribute and it has four possible values, then we’ll split the data four ways and create four separate branches. We’ll follow the ID3 algorithm, which tells us how to split the data and when to stop splitting it. (See http://en.wikipedia.org/wiki/ID3_algorithm for more information.)

Using information theory, you can measure the information before and after the split. Information theory is a branch of science that’s concerned with quantifying information

The change in information before and after the split is known as the information gain. When you know how to calculate the information gain, you can split your data across every feature to see which split gives you the highest information gain. The split with the highest information gain is your best option.

Before you can measure the best split and start splitting our data, you need to know how to calculate the information gain. The measure of information of a set is known as the Shannon entropy, or just entropy for short.

Entropy is defined as the expected value of the information. First, we need to define information. If you’re classifying something that can take on multiple values, the information for symbol xi is defined as

l(Xi) = log2{p(Xi)}.....2.0

where p(xi) is the probability of choosing this class.
To calculate entropy, you need the expected value of all the information of all possible values of our class. This is given by

H = - SUM{from i=1 to n}(p(xi) * log2{p(xi)})

where n is the number of classes.

The higher the entropy, the more mixed up the data is. 

Gini impurity: which is the probability of choosing an item from the set and the probability of that item being misclassified.

What exactly is Information and Entropy?
A decision tree tries to split your data across every feature to see which split gives you the highest information gain. So it is like at the begining data is highly uncertain, as the Decision tree branches out from root to leaf node , at each branch this uncertainity about a class of dependent variable decreases.So in other words Information is nothing but purity or certainity in data where as entropy is nothing but impurity in data.
for more information refer this, it amazing: http://stackoverflow.com/questions/1859554/what-is-entropy-and-information-gain


when do decision tree terminates branching or spliting?
You’ll stop under the following conditions: you run out of attributes on which to split or all the instances in a branch are the same class. If all instances have the same class, then you’ll create a leaf node, or terminating block. Any data that reaches this leaf node is deemed to belong to the class of that leaf node.

If our dataset has run out of attributes but the class labels
are not all the same, you must decide what to call that leaf node. In this situation, you’ll take a majority vote.


The advantage of decision trees over another machine learning algorithm like kNN is that you can distill the dataset into some knowledge, and you use that knowledge only when you want to classify something. 

DRAWBACKS of Decision Tree with ID3 algorithm:
============================================
In case of overfitting. In order to reduce the problem of overfitting, we can prune the decision tree. This will go through and remove some leaves. If a leaf node adds only a little information(entropy change is min), it will be cut off and merged with another leaf.
We’ll investigate this further when we revisit decision trees in chapter 9. In chapter 9 we’ll also investigate another decision tree algorithm called CART. The algorithm we used in this chapter, ID3, is good but not the best. ID3 can’t handle numeric values. We could use continuous values by quantizing them into discrete
bins, but ID3 suffers from other problems if we have too many splits.

SUMMARY ON DECISION TREEs:
A decision tree classifier is just like a work-flow diagram with the terminating blocks representing classification decisions. Starting with a dataset, you can measure the inconsistency of a set or the entropy to find a way to split the set until all the data belongs to the same class. The ID3 algorithm can split nominal-valued datasets. Recursion is used in tree-building algorithms to turn a dataset into a decision tree. The tree is easily represented in a Python dictionary rather than a special data structure.

Cleverly applying Matplotlib’s annotations, you can turn our tree data into an easily understood chart. The Python Pickle module can be used for persisting our tree. The contact lens data showed that decision trees can try too hard and overfit a dataset. This overfitting can be removed by pruning the decision tree, combining adjacent leaf nodes that don’t provide a large amount of information gain. There are other decision tree–generating algorithms. The most popular are C4.5 and CART. CART will be addressed in chapter 9 when we use it for regression.



NAIVE BAYES ALGO:
==================
Naïve Bayes
Pros: Works with a small amount of data, handles multiple classes
Cons: Sensitive to how the input data is prepared
Works with: Nominal values


Bayesian decision theory told us to find the two probabilities:
If p1(x, y) > p2(x, y), then the class is 1.
If p2(x, y) > p1(x, y), then the class is 2.
These two rules don’t tell the whole story. I just left them as p1() and p2() to keep it as simple as possible. What we really need to compare are p(c1|x,y) and p(c2|x,y). Let’s read these out to emphasize what they mean. Given a point identified as x,y, what is the probability it came from class c1? What is the probability it came from class c2?. The problem is that the equation from our friend is p(x,y|c1), which is not the same. We can use Bayes’ rule to switch things around. Bayes’ rule is applied to these statements as follows:

	    P(x,y|Ci)*p(Ci)
P(Ci|x,y) = -----------
		p(x,y)


With these definitions, we can define the Bayesian classification rule:
If P(c1|x, y) > P(c2|x, y), the class is c1.
If P(c1|x, y) < P(c2|x, y), the class is c2.
Using Bayes’ rule, we can calculate this unknown from three known quantities. We’ll soon write some code to calculate these probabilities and classify items using Bayes’ rule.


Statistics tells us that if we need N samples for one feature, we need N**10 for 10 features and N**1000 for our 1,000-feature vocabulary. The number will get very large very quickly.


If we assume independence among the features, then our N**1000 data points get reduced to 1000*N. By independence I mean statistical independence; one feature or word is just as likely by itself as it is next to other words. We’re assuming that the word bacon is as likely to appear next to unhealthy as it is next to delicious. We know this
assumption isn’t true; bacon almost always appears near delicious but very seldom near unhealthy. This is what is meant by naïve in the naïve Bayes classifier. The other assumption we make is that every feature is equally important. We know that isn’t true either. If we were trying to classify a message board posting as inappropriate, we probably don’t need to look at 1,000 words; maybe 10 or 20 will do. Despite the minor flaws of these assumptions, naïve Bayes works well in practice.

Classifying text:
In order to get features from our text, we need to split up the text. But how do we do that? Our features are going to be tokens we get from the text. A token is any combination of characters. You can think of tokens as words, but we may use things that aren’t words such as URLs, IP addresses, or any string of characters. We’ll reduce every piece
of text to a vector of tokens where 1 represents the token existing in the document and 0 represents that it isn’t present.




































