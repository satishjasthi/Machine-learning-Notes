Feature selection(FS) concepts and procedures:

FS can be done using 2 methods:
1.Filter method(FM)
2.Wrapper method(WM)

1.FM:
The outline for Filter method looks somewhat like this,

Take all features
	|
Select a subset of features using tools like(pearson coefficient,ANOVA,LDA(Linear discremenent Analysis) and Chi square test)
	|
Feed the subset features to an Algo 
	|
Measure performance

Tools to measure the correlation between two variables:

Feature\Response		Continuous		Categorical
Continuous		    Pearson's Correlation           LDA
 
Categorical			ANOVA			Chi-Square

Covariance:In probability theory and statistics, covariance is a measure of the joint variability of two random variables.[1] If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, i.e., the variables tend to show similar behavior, the covariance is positive.[2] In the opposite case, when the greater values of one variable mainly correspond to the lesser values of the other, i.e., the variables tend to show opposite behavior, the covariance is negative. The sign of the covariance therefore shows the tendency in the linear relationship between the variables. The magnitude of the covariance is not easy to interpret. The normalized version of the covariance, the correlation coefficient, however, shows by its magnitude the strength of the linear relation.


Pearsonâ€™s Correlation: It is used as a measure for quantifying linear dependence between two continuous variables X and Y. Its value varies from -1 to +1. 

LDA: Linear discriminant analysis is used to find a linear combination of features that characterizes or separates two or more classes (or levels) of a categorical variable.

ANOVA: ANOVA stands for Analysis of variance. It is similar to LDA except for the fact that it is operated using one or more categorical independent features and one continuous dependent feature. It provides a statistical test of whether the means of several groups are equal or not.

Chi-Square: It is a is a statistical test applied to the groups of categorical features to evaluate the likelihood of correlation or association between them using their frequency distribution.

FM can remove multicollinearity among the features, so it has to be removed before apply any algo on ouput of FM.

2.WM:
Outline:

Given all features
	|
Take a subset of them -- feed it to a algo -- measure performance
	||
return best feature subset

the second step operates as a iteration until it finds best subset of features.

Three ways of doing WM:
1.Forward elimination(FE)
2.Backward elimination(BE)
3.Recursive elimination(RE)

In FE you start with no features and then add on features which improves models performance until when the performance doesn't change even with the addition of new features

In BE we do same but we start with all features and go on removing features to get a best subset of features

In RE, it is a greedy algo,  It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the left features until all the features are exhausted. It then ranks the features based on the order of their elimination.




