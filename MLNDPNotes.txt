Friday 17 February 2017 
Source : UCL Course on RL (http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html) by David Silver
Discussion group:https://groups.google.com/forum/#!searchin/csml-advanced-topics/Information$20state%7Csort:relevance

Rewards:
*A reward Rt is a scalar feedback signal
*Indicates how well agent is doing at step t
*The agent’s job is to maximise cumulative reward
Reinforcement learning is based on the reward hypothesis

Reward Hypothesis:
=================
All goals can be described by the maximisation of expected cumlative rewards.

Sequential Decision Making:
=========================
*Goal: select actions to maximise total future reward
*Actions may have long term consequences
*Reward may be delayed
*It may be better to sacrifice immediate reward to gain more
long-term reward

Examples:

1.A financial investment (may take months to mature)
2.Refuelling a helicopter (might prevent a crash in several hours)
3.Blocking opponent moves (might help winning chances many
moves from now)

Agent and Environment:
======================

See Image UCL_RL1
here brain is the agent,it gets the observation and reward as input and it has to predict the action.

SEE image UCL_RL2

here environment is the one that creates observations and rewards for agent,the agent has no control over the observations and rewards because it is controlled by Environment, in other words we cannot control the behaviour of Environment.The agent can influence the environment by performing action.


the stream of experience of the agent is called its history
ie the sequence of observations,actions and rewards
Ht = A1,O1,R1,.......At,Ot,Rt
where A = action,O = observation,R = rewards
At = is the action taken at time t sec
A1 = is the action taken at time 1 sec

Agent's algorithm maps from history to action.ie agent's algo use it's history to decide what its next action should be

Environment uses the histroy to create next observations and rewards.

History is not that useful because it enormous and we cannot go back every time to look for something.For this we use something called as State 

State is like the summary of information used to determine what happens next.

formally state is a function of History:
 St = f(Ht)

Environment state:S(e,t) is the state of the environment which determines which observation and reward it has to choose for agent next.It's not visible for agent.Agent's algo doesn't depend on environment state

Agent state:The agent state S(a,t) is the agent’s internal representation basically summarises what it has seen till now ,to help agent to choose what happens next.
It can be any function of history:

S(a,t) = f(Ht)


Information/Markov State:An information state (a.k.a. Markov state) contains all useful information from the history.

A state St is Markov if and only if
P[St+1 | St] = P[St+1 | S1, ..., St]

ie Prob of next state given your previous state = Prob of next state given all the other previous states


“The future is independent of the past given the present”
H1:t → St → Ht+1:∞
This means you'll only need St(the present state) and you can throw away the histroy.Because the St alone can capturaise all future observations,rewards and actions.

Once the state is known, the history may be thrown away
i.e. The state is a sufficient statistic of the future
The environment state S(e,t) is Markov.The history Ht is Markov


SEE IMAGE UCL_RL3
the image above shows how our state predicts action using histroy

Fully Observable Environments:here agent gets to see environment state 
So

Ot = S(a,t) = S(e,t)

Ot = observation

and agent state = environment state, formaly is a MDP(markov decision process)


Partial observability: agent indirectly observes environment:
1.A robot with camera vision isn’t told its absolute location
2.A trading agent only observes current prices
3.A poker playing agent only observes public cards

Now agent state != environment state
Formally this is a partially observable Markov decision process
(POMDP)

Agent must construct its own state representation S(a,t) using following methods:
eg: 1.Complete history: S(a,t) = Ht, it uses complete history
2.Beliefs of environment state S(a,t) = (P[S(e,t) = S1].......P[S(e,t) = Sn]) 
3.Recurrent neural network S(a,t) =  σ(S(a,t-1)*Ws + Ot*Wo)
where σ = learning rate.

Inside an RL agent:
=================

An RL agent may include one or more of these components:
1.Policy: agent’s behaviour function
2.Value function: how good is each state and/or action
3.Model: agent’s representation of the environment


Policy:
*A policy is the agent’s behaviour
*It is a map from state to action, e.g.
*It can be a Deterministic policy: a = π(s)
* or a Stochastic policy: π(a|s) = P[At = a|St = s]


Value function:
*Value function is a prediction of future reward
*Used to evaluate the goodness/badness of states
*And therefore to select between actions, e.g.

vπ(s) = Eπ[Rt+1 + (γ)Rt+2 + (γ**2)Rt+3 + ... | St = s)

vπ = value function of policy pi(π)
Eπ is the expected value of policy π
γ = is the discount factor which is between 0 and 1, you can see the value R decreases with increase in power of γ, which signifies that we care more about immediate rewards than far future rewards

Model:

*A model predicts what the environment will do next from agent's point of view
*predictive model:P predicts the next state
*reward model: R predicts the next (immediate) reward, e.g.

P(a,s,st) = P[St+1 = st| St = s, At = a]
		next    previous  previous
	       state     state    action

R(a,s)  = E [Rt+1 | St = s, At = a]
	     next   previous  previous
	   reward    state	action


SEE IMAGES UCL_RL4 to UCL_RL7

Categorizing RL agents:
======================
Value Based agents :
* uses No Policy (Implicit)
* uses Value Function

Policy Based:
*uses Policy
*uses No Value Function

Actor Critic:
*uses Policy
*uses Value Function

Model Free:
*uses Policy and/or Value Function
*uses No Model

Model Based:
*uses Policy and/or Value Function
*uses Model

Problems within RL:
1.Learning and Planning

Two fundamental problems in sequential decision making

Reinforcement Learning:
1.The environment is initially unknown
2.The agent interacts with the environment
3.The agent improves its policy

Planning:
1.A model of the environment is known
2.The agent performs computations with its model (without any
external interaction)
3.The agent improves its policy
4.a.k.a. deliberation, reasoning, introspection, pondering,
thought, search

SEE IMAGE UCL_RL8,UCL_RL9

2.Exploration and Exploitation:
*Reinforcement learning is like trial-and-error learning
*The agent should discover a good policy
*From its experiences of the environment
*Without losing too much reward along the way
*Exploration finds more information about the environment
*Exploitation exploits known information to maximise reward
*It is usually important to explore as well as exploit


SEE IMAGE UCL_RL10

3.Prediction and Control

*Prediction: evaluate the future
	-Given a policy

*Control: optimise the future
	-Find the best policy

SEE IMAGE UCL_RL11 and 12


MARKOV DECISION PROCESS:
MDP formally describes an environment for reinforcement learning, in other words it helps in describing what the environment is for agent, so that agent can take appropriate tasks.And in MDP environment is fully observable.
The current state completely characterises the process, ie the current state gives the agent an idea about how to unfold the future sequences of actions it has to take. 

Almost all of the RL problems can be formalised as MDPs,
Ex: 1. Optimal control primarily deals with continuous MDPs
2.Partially observable problems can be converted to MDPS(if you have a proper fundamentals of MDP then any Partially observable problems can be convereted into MDP).
3.Bandits are MDPS with one state(this is like you are provided with a set of actions and you'll be chosing only one state for which you'll be rewarded,Ex:those ads that are suggested for you while you are googling.)

Markov Property: is the key for MDP

State transition probability and State transition Matrix:
For a markov state s and succesor state s', the state transition probability is defined by

Pss' = P[St+1 = s'|St=s]

State transition matrix P defines transition probabilities from all states s to all sucessor states s'.ie it encomposes the probability of a state s1 and all other states,when you start from s1.like this it includes probabilities for each state as starting positions and other states as succesor state

Markov process: is a memoryless random process ie a sequence of random states S1,S2....with the markov property

Def: A markov process(or Markov chain) is a Tuple (S,P)
where S is a finite set of states and P is the transition probability
Pss' = P[St+1=s'|St=s]


Samples of MDP

See IMAGES UCL_RL13 UCL_RL14 
 
Markov Reward Process:A Markov reward process is a Markov chain with values or rewards

Defin: A Markov Reward Process is a tuple (S,P,R,γ)
S is a finite set of states
P is a state transition probability matrix,
Pss' = P [St+1 = s'| St = s]
R is a reward function, Rs = E [Rt+1 j St = s]
γ is a discount factor, γ 2 [0; 1]


Reward function is the one which calculates the immediate rewards of actions.

see IMAGE UCL_RL15

Return/Goal:

Definition:
The return Gt is the total discounted reward from time-step t.
Gt = R(t+1) + γR(t+2) + .... = SUM(k = 0 to infinity ){(γ**k)*R(t+k+1)}

this calculates the sum of all future rewards from current state to inifinity

*The discount γ 2 [0,1] is the present value of future rewards
*The value of receiving reward R after k + 1 time-steps is γkR.
*This values immediate reward above delayed reward.
	*γ close to 0 leads to "myopic" evaluation ie you only care about R(t+1)
	*γ close to 1 leads to "far-sighted" evaluation ie you'll consider sum of all R's

Why discount?
Most Markov reward and decision processes are discounted. Why?
 *Mathematically convenient to discount rewards
 *Avoids infinite returns in cyclic Markov processes(like the facebook loop in Iamge UCL_RL15)
 *Uncertainty about the future may not be fully represented
 *If the reward is financial, immediate rewards may earn more
 interest than delayed rewards
 *Animal/human behaviour shows preference for immediate
 reward
 *It is sometimes possible to use undiscounted Markov reward
 processes (i.e. γ = 1), e.g. if all sequences terminate

Value Function:
The value function v(s) gives the long-term value of state s

Definition:
The state value function v(s) of an MRP is the expected return
starting from state s
v(s) = E [Gt j St = s]

SEE image UCL_RL16
here the value of value function for state C1 is Expectation of all those samples that start from C1 ie average of all those states that start from C1

the role of discount factor γ
See IMAGE UCL_RL17,UCL_RL18
as you can see when γ=0 the value function only cares about the immediate rewards of the state,however when γ = 1 it cares about the immediate as well as future states, which can infered from the values of value function in red color in those photos

Bellman's Equation for MRP:

what bellman's equation basically says is that if you move from your present state a new state what is the immediate reward + how good is to be in that state.

The value function can be decomposed into two parts:
immediate reward Rt+1 and discounted value of successor state γv(St+1)
v(s) = E [Gt j St = s]
= E [Rt+1 + γRt+2 + γ2Rt+3 + ...| St = s]
= E [Rt+1 + γ (Rt+2 + γRt+3 + ...) | St = s]
= E [Rt+1 + γGt+1 | St = s]
= E [Rt+1 + γv(St+1) | St = s]...Bellman's equation

See IMAGE UCL_RL19 and 20 where Pss' is the transition probability.

Bellman Equation in matrix form:
See image UCL_RL21

Solving the Bellman Equation:
*The Bellman equation is a linear equation
*It can be solved directly:

v = R + γPv
(I − γP) v = R
v = (I − γP)**(−1) R...1.2

*for solving 1.2, Computational complexity is O(n3) for n states
*Direct solution only possible for small MRPs ie that matrix (I − γP)**(−1)
*There are many iterative methods for large MRPs, e.g.
1.Dynamic programming
2.Monte-Carlo evaluation
3.Temporal-Difference learning


Markov Decision Process: 
A Markov decision process (MDP) is a Markov reward process with
decisions. It is an environment in which all states are Markov.

Definition:
A Markov Decision Process is a tuple (S,A,P,R,γ)
S is a finite set of states
A is a finite set of actions
P is a state transition probability matrix,
P(a,s,s') = P [St+1 = s'| St = s; At = a]
R is a reward function, R(a,s) = E [Rt+1 | St = s, At = a]
γ is a discount factor γ 2 [0, 1]

Policies:
Definition: 
A policy π is a distribution over actions given states,
π(ajs) = P [At = a j St = s]
ie it attaches probability to all possible actions available for a given state,and the agent is the one which will be weighing this probability values,hence the policies are completely under the control of the agent.

A policy fully defines the behaviour of an agent
Even though policy try to get highest immediate reward their is no reward term in policy formula because the present state 's' fully characterises the evolutionf from present state onwards.We care only about future rewards and not the past rewards.

MDP policies depend on the current state (not the history)
i.e. Policies are stationary (time-independent),
At ∼ π(·|St); forall t > 0
ie Policies don't change with time

Given an MDP M = (S,A,P,R,γ) and a policy π

The state sequence S1,S2... is a Markov process (S,Pπ)
One connection between MRP and MDP is that we can always recover an MRP from MDP.And Any policy that we choose for an agent is an Markov chain


The state and reward sequence S1, R2, S2,... is a Markov
reward process (A,Pπ,Rπ,γ)
where
where Pπ and Rπ are the transition prob/ dynamics  and Reward functions which are defined as the follows
Pπ(s,s') = SUM(a e A)π(a|s)*P(a,s,s') ie the average of all the transition dynamics that we are gonna do from that particular state.

Rπ,s = SUM(a e A)π(a|s)*R(a|s)

Value function:
Types:
1.State value function
2.Action value function

Vπ(s) = states how good is it to be in state s following the policy π.
Eπ = is the expectations when we sample all the actions according to policy π.
Def of state value function:
The state-value function vπ(s) of an MDP is the expected return
starting from state s, and then following policy π

vπ(s) = Eπ [Gt | St = s] # tells you how good it is to take a particular function

Def of action value function:
The action-value function qπ(s, a) is the expected return
starting from state s, taking action a, and then following policy π

qπ(s, a) = Eπ [Gt | St = s, At = a] # tells you how good it is to take a particular action

Updated Bellman's equation:
The state-value function can again be decomposed into immediate
reward plus discounted value of successor state,

vπ(s) = Eπ [Rt+1 + γ*Vπ(St+1) | St = s]

The action-value function can similarly be decomposed,

qπ(s, a) = Eπ [Rt+1 + γ*qπ(St+1, At+1) | St = s, At = a]

See image UCL_RL21 to 26

those value or calulation in red in image 26 can be done using MRP
ie The Bellman expectation equation can be expressed concisely
using the induced MRP,

vπ = Rπ + γPπvπ

with direct solution

vπ = (I − γPπ)−1 Rπ

Finding the optimal value function:
The optimal state-value function v∗(s) is the maximum value
function over all policies

v∗(s) = max_π(vπ(s))

The optimal action-value function q∗(s, a) is the maximum
action-value function over all policies

q∗(s,a) = max_π(qπ(s, a))

basically the main unkown in MDP is finiding q∗(s,a),if we find it then MDP is solved.

The optimal value function specifies the best possible
performance in the MDP.
An MDP is \solved" when we know the optimal value fn.

SEE IMAGE UCL_RL 29 and 30

Finding optimal policy:there can be more than one optimal policy  

Define a partial ordering over policies
consider 2 policies π and π', π is the optimal policy if

π ≥ π' if vπ(s) ≥ vπ'(s),forall s

Theorem:
For any Markov Decision Process
1.There exists an optimal policy π∗ that is better than or equal
to all other policies, π∗ ≥ π; forall π
2.All optimal policies achieve the optimal value function,
vπ∗(s) = v∗(s)
3.All optimal policies achieve the optimal action-value function,
qπ∗(s; a) = q∗(s; a)

how do we find the optimal policy? 

An optimal policy can be found by maximising over q∗(s, a),

π∗(a|s) = ( 1 if a =  a = argmax(a e A){ q∗(s, a)} , otherwise 0)

*There is always a deterministic optimal policy for any MDP
*If we know q∗(s, a), we immediately have the optimal policy

SEE IMAGE UCL_RL31

How do find q* ?
Bellman's optimality equation tells us how to find it.

SEE image UCL_RL32,33,34,35,36
here the value function is calculated as a max of action function instead of average of action function.

Solving the Bellman Optimality Equation:
*Bellman Optimality Equation is non-linear
*No closed form solution (in general)
*Many iterative solution methods are avialable like:
1.Value Iteration
2.Policy Iteration
3.Q-learning
4.Sarsa


PLANING BY DYNAMIC PROGRAMMING:
===============================

Dynamic Programming?
*Dynamic means that the problem has a sequential or temporal component and some components of the problem changes with time.
*programming is process of optimising a policy

Together it's a optimization problem for sequential problems

Ex: c.f linear programming

Where can we use Dynamic programming?
It can be applied in any problem where the main problem is divided into sub problems and the solutions for these sub problems are found and we integrate all these to solve the main problem.


DP(Dynamic programming need 2 main components to implement it:)
ie DP is a very general solution method for problems which have 2 properties:

1.optimal substructure:(means that the given complex problem can be divided into subproblems and find the optimal solutions for these problems and these solutions will help you find the optimal solution for the main complex problem.)
	*Principle of optimality applies.

	*Optimal solutions can be decomposed into subproblems

2.Overlapping subproblems:(means that the subproblems that occur will occur again and again,by solving those subproblems we can solve the main problem much more effectively than directly solving the main problem )
	*Subproblems reoccur many times 
	*Solutions can be cached and reused.ie by solving the small problems we can reuse them in solving much complex problems,by using these like.

3.Markov Decision process supports both above properties:
	* Bellman's equation gives recursive decomposition(ie optimal substrucutres)
	* Value functions stores and reuses solutions, ie for all the states value function has already calculated value so for any given state it will recall the state's value from its cache 

 Dynamic programming assumes full knowledge of the MDP, ie it is used for palnning(where the environment is completely known) in an MDP

For predicition problem: 

here Input to the agent is : MDP ie (S,A,P,R,γ) and a policy or 
a MRP (S,Pπ,Rπ,γ)
and Output: value function Vπ

For control problem:
Input: MDP (S,A,P,R,γ)
output: optimal value function Vπ

So first we'll solve the prediction problem which'll give us the value function for a policy and then we'll move on to control problem where we'll find the optimal policy.we'll use these two steps iteratively to find the optimal policy. 

Applications of DP:
* Scheduling algorithms
* String algorithms (eq: sequence alignment)
* Graph algorithms (eq: shortest path algorithms)
* Graphical models (eg: Viterbi algorithm)
* Bioinformatics(eg: lattice models)


POLICY EVALUATION:
==================

Problem: evaluate a given policy π
Solution:Iterative application of Bellman's expectation backup

process: Initially we begin with some random value function V1 whose value are random they can be set to zero, then we apply bellman's expectation equation to update the value of V1 to V2 and this process repeates until we reach the optimal value of V 

The way we update this is using synchronous backups(because all states are updates together): 
*in every iteration we are gonna sweep through all the states of MDP
* at each iteration K+1
* for all states s e S
* Update Vk+1(s) from Vk(s')
where s' is a successor state of s

there is also asynchronous backup iterative process

Take a IMAGE of the side after synchronous backup


POLICY ITERATION:
================
*given a policy π
	* Evaluate the policy π :
		Vπ(s) = E[R(t+1) + γ*R(t+2) + ...|St = s]
    

*Improve the polciy by acting greedly with respect to Vγ
π' = greedy(Vπ)
you'll look around your state and pick the best value that's what greedy means.

*In general we need more iterations of improvement/evalution for complex MDPs
*But this procees of policy iteration always convergs to π* ie optimal policy   




































