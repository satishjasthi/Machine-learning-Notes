*** Source: Udacity ***

Supervised learning is like a induction function where the machine learns by observing examples which are labeled as right and wrong.(induction basically mean identifying the patterns first and then creating a function which can generalise that pattern to the future inputs)

*Reinforcement learning is like learning from experience

*OPTIMIZATIONS PROBLEMS IN 
1. Supervised learning: to label data well
2. Unsupervised learning: to cluster data well
3. Reinforcement learning: to get best behaviour scores


TERMS IN CLASSIFICATION:
Instances: Input
Concept: an idea of mapping input to output
target concept: it is the actual required function which mapps from inputs to outputs
Hypothesis class : is a set of all possible functions
Sample: training set
candidate: Concept that can be a target concept


DECISION TREES:

		O
	   0	     0
	0     0   0     0

This represents the decision tree, the circle represents the decision nodes

Decision tree always moves from top to bottom

The goal of asking questions in Decision trees is to narrow down the space to a particular point


Decision tree learning:
1.Pick the best attribute(best is the one that narrows the scope by a greater amount )
2. Ask a question about it
3.Follow the answer path
4.Go to 1
 repeate 1 to 4 until you end up with what you want


Decision tree is linear if the number of nodes is equal to number of levels of tree

If the tree grows exponentially then decision tree is exponential

Information gain:is simply a mathematical way to capture the amount of information gain that we want to gain by picking particular attribute.
In other words information gain reduces the amount of randomness when selecting a specific attribute.Maximizing information gain means reducing the amount of randomness while choosing the attributes.

Gain(S,A) = Entropy(S) - SUM(v){(|Sv|/|S|)*Entropy(Sv)}
where S: is the collection of training examples
A: is a particular attribute
V:labels of attributes

entropy is maximum when the mixture has all impurities in equall proportions.And it is least when we have no impurities, ie we have only one category.


Entropy of a value = -Sum(from 1 to v){p(V)*log(P(V))}

Once we have understand about information gain and entropy which helps us understand how exactly a decision tree splits.

Now when we have n attributes for a given predictor then we can have n decision  trees possible, but how can we choose the optimal tree out of these n trees. For which we use
something called BIAS

We have 2 kinds of BIAS:
1.Restricition BIAS(H): it is a set which includes only functions that are decision trees.ie the mapping between X & Y can be done by any function(infinetly many) but we want to consider only the subset of them which are purely decision trees.

2.Reference BIAS(h): is a subset of H,ie out of possible decision trees which one are we choosing.

INDUCTIVE BIAS:(which is a reference bias)
what makes a good decision tree:
1. Good splits at top
2.Correct over incorrect
3.Shorter tree are better than longer trees

We can provide continuous attributes to decision tree as inputs

When do we stop spliting decision tree?
1. We can use PRUNING method
2. Use CROSS VALIDATION METHOD to find the decision tree that has least CV error


LINEAR REGRESSION:
Cross validation set: is a part of training set which is used to test whether a chosen model is overfitting or perform well.



*.Naive Bayes algorithm: 
*. It's mainly used for classification problems related to texts.
*.It is called Naive Bayes because it doesn't actually understand the sentences but it classifies them purely on the basis of their frequency.
Pros of NB algo: it's computationally cheap for large datasets
 

SUPPORT VECTOR MACHINES(SVM):
*.It basically creates a line between two class of the data ,something like a decision boundary but in this case it is called hyperplane

*. In SVM margin is nothing but the minimum distance between the hyperplane and the two class of the data it separates

|    *---|  *
|*  *    |
| *      |--*       ---- these distances are the margins
|  *     |  * 
|*       |  * 
| **     | * 
|________|______

*.SVM gives first preference to correct classification and then maximizes the margin

*. Adding new features to SVM can help it to linearly classify non linear data

*.SVM is also called large margin classifiers & it is highly sensitive to outliers when c or 1/lambda(regularization parameter) is very high( because it tries to minimize margin)

*. SVM and Kernels work together in a better way because SVM is mathematically & computationally optimised to use Kernels, eventhough you can use kernels with linear regression ,we won't use because it makes the algo slow & computationally expensive.

SVM PARAMETERS: C or 1/lambda
1Kernels: 

2.C:
2.1.Large C value : lower bias, high variance  (because lambda is small)
2.2.Small C value : Higher bias, low variance  (because lambda is large)

3.While using Gaussian kernel:
-Large sigma**2 value :Features fi vary more smoothly
higher bias,lower variance
-Small sigma**2 value: Features fi vary less smoothly .
Lower bias,high variance.

4.gamma parameter also plays a vital role in high variance & high bias

SVM is not a good algo to go when the dataset is really large and filled with a lot of noise,because it would be computationally expensive and very slow that is where we can use Naive Bayes algo.


OUTLIERS:
there are 2 kinds of them:
1. which we can ignore since they'll be caused by data entry error or sensor malfunctioning
2.which we have to pay special attention to them because they can give a vital anomaly in data.

*.Outlier removal:
1.Train on data
2.Remove points with largest residual error,fraction of data that would be removed will be around 10%.(but it vary depending on applications)
3.re-train the model on remaining data

*. Identifying and cleaning away outliers is something you should always think about when looking at a dataset for the first time

Neural Networks:
*.In order to train a NN you can use either:
1.Percepton rule or
2.Gradient descent

single neuron/perceptron: acts like a activator

for example X1,X1,X3 are inputs and W1,W2,W3 are respective weights and theta is the threshold value of the perceptron.
So if X1W1 + X2W2 + X3W3 >= theta then the perceptron will give 1 else 0 as output

In general 
if a = SUM(i = 1 to k){XiWi}

Y = 1 if a>=theta
Y = 0 if a< theta

perceptron are linear functions which always creates hyperplanes as decision boundaries

To find the optimal weights for perceptrons using training data set we have two methods:

1.perceptron rule
2.Gradient descent rule

1.Percepton rule: can be used when the data is linearly seperable(ie they can be completely separated by a straight line or a plane.)


Wi = Wi + dWi // Wi's are the weights and dWi is delta Wi
dWi = alpha * (y - y^)Xi // y is the actual output & y^ is the predicted value and Xi is the individual X values,aplha- learning rate

y^ = (SUM(i=1 to n){WiXi} >= 0)

Perceptron method is really good for linearly separable data,because it can find the solution in very few steps.

when we are using gradient descent in NN to train the model we use a differential function like sigma to make decision between 0 & 1. Activation values are feed to sigmoid function to get Y^ between 0 & 1.

Backpropagation in NN: is nothing but passing the errors in weights back to the NN to reduce them & therby improving ouput.

Gradient descent suffers from local optimum: to free it from this we have some methods for optimizing weights:
*momentum: use the concepts of momentum
* higher order derivatives
* randomization optimization
* penalty for complexity: model become complex:
	when we add more number of nodes,layers or large weights


INSTANCE BASED LEARNING:

KNN algo:





UNSUPERVISED LEARNING:
======================

*Clustering:
	Single link clustering(SLC):
How does SLC works: 
1. given a set of N points
2. Initiate the algorithm by considering each point as a single cluster
3. now calculate the distance between each of these clusters, those clusters which are nearer to each other will merge into a single cluster.
4.repeat 3 until N-K times,till you reach K clusters, where k is the number of clusters we need

SLC always considers the closest distance as a distance metric.

SLC is determinitic







=========================================================================
Source: Udacity
INTRO TO ALGORITHMS:

See IMAGE PD1

Nodes: are the vertices of the graph like SS,DH,JR
Edges/links: are the connections between the nodes
degree of a node: is the number of edges connected to a node
in image PD1 DH node has a degree of 4

Eulerian Path: is the path covered when you move from one node to other in a graph without going through the same path twice.


IMAGE PD2 shows the eulerian path and degree of nodes

It can be seen from the PD2, that all the nodes in the graph except starting and ending nodes have even degree. whereas starting and ending nodes has odd degree.

When you have starting and ending points at two different points,then the above rule applies

But if ending and starting point are the same then it is called Eulerian Tour which is a special kind of Eulerian path


Why do we need Algos:
* to get a quick response from the programs and applications that we run
* It basically helps your programm to fly instead of crawing using its clever techniques and math
* to optimise your program so that it doesn't waste time on unnecessary things

 
How do math helps us in learning algos?
1.It formalises what you want to do
2.analyse the correctness of your solution
3. Analyse the solution's efficiency(it refers to many like time, memory,energy usage)

Bitwise operators in python:
x>>y : means dividing x by 2**y or we can say that it half the number if it is even. if the number is odd then subtract 1 from it and half the number
x<<y : means multiply x by 2**y,this doubling number

Measuring the time taken by a algo using following assumptions as rules:
1.Simple statements takes unit time
Ex: x += 1

2. Sequence of simpe statements = the sum of unit time for each statement

Ex: if y=4; z = z/2 , takes 2 unit time

3.Loop takes time equal to the body x iterations

Ex: for i in range(4):
	print hello
this program takes 4 unit times


 














================================================
PROJECT: FINDING DONORS

DATA TRANSFORMATION IN STATISTICS:
Guidance for how data should be transformed, or whether a transformation should be applied at all, should come from the particular statistical analysis to be performed. For example, a simple way to construct an approximate 95% confidence interval for the population mean is to take the sample mean plus or minus standard error units. However, the constant factor 2 used here is particular to the normal distribution, and is only applicable if the sample mean varies approximately normally. The central limit theorem states that in many situations, the sample mean does vary normally if the sample size is reasonably large. However, if the population is substantially skewed and the sample size is at most moderate, the approximation provided by the central limit theorem can be poor, and the resulting confidence interval will likely have the wrong coverage probability. Thus, when there is evidence of substantial skew in the data, it is common to transform the data to a symmetric distribution before constructing a confidence interval. If desired, the confidence interval can then be transformed back to the original scale using the inverse of the transformation that was applied to the data.

Data can also be transformed to make it easier to visualize them. For example, suppose we have a scatterplot in which the points are the countries of the world, and the data values being plotted are the land area and population of each country. If the plot is made using untransformed data (e.g. square kilometers for area and the number of people for population), most of the countries would be plotted in tight cluster of points in the lower left corner of the graph. The few countries with very large areas and/or populations would be spread thinly around most of the graph's area. Simply rescaling units (e.g., to thousand square kilometers, or to millions of people) will not change this. However, following logarithmic transformations of both area and population, the points will be spread more uniformly in the graph.

A final reason that data can be transformed is to improve interpretability, even if no formal statistical analysis or visualization is to be performed. For example, suppose we are comparing cars in terms of their fuel economy. These data are usually presented as "kilometers per liter" or "miles per gallon." However, if the goal is to assess how much additional fuel a person would use in one year when driving one car compared to another, it is more natural to work with the data transformed by the reciprocal function, yielding liters per kilometer, or gallons per mile.


Precision and recall:
precision (also called positive predictive value) is the fraction of retrieved instances that are relevant, while recall (also known as sensitivity) is the fraction of relevant instances that are retrieved. Both precision and recall are therefore based on an understanding and measure of relevance.

In other words precision is how many of them are correct and recall is how many of them are correct out of what algo thought as correct

Precision = TP/(TP+FP)
recall = TP/(TP+FN) ,out of all correct ones how many can it recall.




Bayesian learning:
* learn the best hypothesis given data and some domain knowledge

* Bayes rule: 
	P(D|h)*P(h)
P(h|D) = ---------------
	P(D)
h - hypothesis; D - given some data 

P(D) - is the prior or the data
P(D|h) - labeling data given hypothesis,which is nothing but ML algo itself.
P(h) - is the prior(domain knowledge) about hypothesis,that which hypothesis is best sutiated out of all available ones


* how does it work:
for each h that belongs to H
calculate P(h|D) = P(D|h)*P(H)/P(D)

output : h = argmax P(h|D)...3.0, select the one with the highest prob

we can actually approximate P(h|D) = P(D|h)*P(H),
by negelecting P(D) ,because any way we are looking for max value of h in 3.0 and P(D) doesn't matter much, in most of the cases it is not possible to obtain prior on data.

So ,P(h|D) ~= P(D|h)*P(H) ...3.1
when we are using 3.1 we call it MAP(maximum a posterior),because here we have a porior for hypothesis.That here we give more importance to some hypothesis rather than other.

However in ML(maximum liklehood),we have no prior for hypothesis,because we assume that all hypothesis has same chances of being selected,in which

P(h|D) ~= P(D|h), with no priors for hypothesis

unless number of hypothesis(h) are fewer or limited this approach would be impractical

Bayes learning in action:
* 3 major assumptions:
	*given data points {(Xi,di)} as noise free examples of some function c, here noise free means all the points in the data can be expressed using function c
	* C belongs to(e) our Hypothesis set
	* all h's e H are equally likely

* know given a hypothesis how can we know the probability of a chosing that particular hypothesis given data

we know 

	P(D|h)*P(h)
P(h|D) = ---------------
	P(D)
P(h) = 1/|H|, |H| is the size of hypothesis set, which contains all possible hypothesis.

now P(D|h) = 1 if di = h(Xi) for all Xi,di e D ie dataset , nothing but predicted == actual value of independent variable

P(D|h) = 0 otherwise

P(D) = Sum(from h0 to hN){P(D|hi)*P(hi)}

Since we consider that all examples folloe function c

P(D) = Sum(hi e Versionspace,D){1 * (1/|H|)} = versionspace/|H|


P(h|D) = 1/versionspace ,it is only true for h e version space
 
Consider an Example:(https://classroom.udacity.com/nanodegrees/nd009/parts/596c7dc6-8049-4785-adfe-7c83ca19b00f/modules/5c2f3b47-b791-46a7-88eb-34a0753665e6/lessons/5462070314/concepts/4733385560923#)

dataset:
X	d
1	5
3	6
11	11
12	36
20	100

here d is the output value
let's say d = k*x, where k e {1,2,3....}
let the candid hypothesis be an identity function

h(x) = x

let the prob of prediciting each of the di is 1/2**k

then P(1/2**k) = 1/2**5 * 1/2**2 * 1/2 * 1/2**3 * 1/2**5 = 1/65536

Minimum Description length

We know h_map =  argmax P(D|h)*P(h)
	      =  argmax[lg(P(D|h) + lg(P(h))]
	      =  argmin[-lg(P(D|h) - lg(P(h))]...3.4
to max h_map we must min 3.4

-lg(P(D|h) ~ length(D|h)
-lg(h) ~ length(h), this is nothing but the min value of lg(h) gives the optimal hypothesis for given data

Bayes Inference:

Conditional Independence:
X is conditionally independent of Y given Z, if the probability distribution governing X is independent of the values of Y given the values of Z, that is if

for all X,Y,Z P(X=x|Y=y, Z=z) = P(X=x| Z=z)
ie P(X|YZ) = P(X|Z)

which is similar to 
P(X,Y) = P(X)*P(Y) if X & Y are independent

P(X,Y) = P(X|Y).P(Y)
p(X|Y) = P(X)


Belief Networks or Bayes Networks or Bayesian Networks or Graphical Models:






===================================================
ENSEMBLE LEARNING: BOOSTING

Ensemble learning algo's combines simple rules which cannot tackle the problem on their own but when they are combined together, they form a complex rule which can solve the problem efficaciously

========================================================================
Source:https://onlinecourses.science.psu.edu/stat414/node/97

Probability Density Functions:A continuous random variable takes on an uncountably infinite number of possible values. For a discrete random variable X that takes on a finite or countably infinite number of possible values, we determined P(X = x) for all of the possible values of X, and called it the probability mass function ("p.m.f."). For continuous random variables, as we shall soon see, the probability that X takes on any particular value x is 0. That is, finding P(X = x) for a continuous random variable X is not going to work. Instead, we'll need to find the probability that X falls in some interval (a, b), that is, we'll need to find P(a < X < b). We'll do that using a probability density function ("p.d.f."). 

Example
Even though a fast-food chain might advertise a hamburger as weighing a quarter-pound, you can well imagine that it is not exactly 0.25 pounds. One randomly selected hamburger might weigh 0.23 pounds while another might weigh 0.27 pounds.  What is the probability that a randomly selected hamburger weighs between 0.20 and 0.30 pounds? That is, if we let X denote the weight of a randomly selected quarter-pound hamburger in pounds, what is P(0.20 < X < 0.30)?

Now, you could imagine randomly selecting, let's say, 100 hamburgers advertised to weigh a quarter-pound. If you weighed the 100 hamburgers, and created a density histogram of the resulting weights, perhaps the histogram might look something like this: 

SEE IMAGE GD10


In this case, the histogram illustrates that most of the sampled hamburgers do indeed weigh close to 0.25 pounds, but some are a bit more and some a bit less. Now, what if we decreased the length of the class interval on that density histogram? Then, the density histogram would look something like this:


SEE IMAGE GD11

Now, what if we pushed this further and decreased the intervals even more? You can imagine that the intervals would eventually get so small that we could represent the probability distribution of X, not as a density histogram, but rather as a curve (by connecting the "dots" at the tops of the tiny tiny tiny rectangles) that, in this case, might look like this:

SEE IMAGE GD12

Such a curve is denoted f(x) and is called a (continuous) probability density function.

Now, you might recall that a density histogram is defined so that the area of each rectangle equals the relative frequency of the corresponding class, and the area of the entire histogram equals 1. That suggests then that finding the probability that a continuous random variable X falls in some interval of values involves finding the area under the curve f(x) sandwiched by the endpoints of the interval. In the case of this example, the probability that a randomly selected hamburger weighs between 0.20 and 0.30 pounds is then this area:


SEE IMAGE GD13

Now that we've motivated the idea behind a probability density function for a continuous random variable, let's now go and formally define it.


SEE IMAGE GD14 15 16


Reinforcement learning:(RL)
======================

In case of supervised learning y = f(x)
here given x ,ML algo will try to develop a function f which can predict y

In case of unsupervised learning f(x)
here given x, ML algo will try to find the function to cluster the data into similar category.

In case of RL y,z = f(x)
here given x & z, ML algo will try to find the function using x and z , to predict y


RL is one of the mechanisms to perform Decision Making.

Markov's Decision Process(MDP):
=======================
States:(S) are like a sample space which involves all possible states possible for any given situation.

Actions A(s) or A: are the things that we can perform in any given states.

Transition Model : T(s,a,s') ~ Prob(s'|s,a)
transition is like the physics that governs your actions or your entire play in any given world.
It consists of three parameters
s - initial state
a - action taken
s' - finial state, which can be same as initial state 

T(s,a,s') ~ Prob(s'|s,a) states that T(s,a,s') is the probability of you ending up at state s' given you have taken action 'a' from state 's' .

Markovian property: states that only the  present matters and the rules of the world are stationary.

Reward R(s),R(s,a),R(s,a,s'): is some scalar value that you'll get when you are in a particular state.


MDP is like a problem and the solution to this problem is something called "Policy"

Policy : PI(s) ----> a ie for any given state that you are in it gives you the action you need to perform.

Policy* or PI* is the optimal policy which maximises the long term expected reward

So if consider RL as Supervised learining it would look something like
<s1,a1>,<s2,a2>..........<sn,an> ie you are given n pairs of states and actions based on which the ML algo will devlop f which is PI in Rl

however what exactly happens in RL is you are given 
<s1,a1,r1>,<s2,a2,r2>..........<sn,an,rn>  ie you are given n pairs of states,actions & rewards based on which the RL algo will devlop f which is PI in Rl and the 'r' is equivalent 'z' that we have discussed in RL definiton.
ie y,z = f(x) 

Policy is the one that states you what action you must take at any given state you are in.

Policy is different from plan making because it doesn't tell you what actions you must take to reach the goal but it only states what immediate action you should take given your state

Temporal Credit assignment problem: is where your algo will learn to choose optimal actions from late rewards in contrast with immediate rewards like in your Supervised learning.

Ex: 
SEE IMAGE Temporal credit problem

here in case of SL you'll be rewarded at each state whereas in RL you'll be rewarded at reward state

The value of the reward is the one that plays a vital role in MDP to reach the final reward position as quickly as possible or as slowly as possible.


Sequence of rewards:

* In case of RL if we have infinite time to reach from start till goal then it is said to have inifinite horizon and policy for the game is defined as

Pi(s) ---> a ; ie Policy(state) = action
here the action wrt to a given state remains same no matter at what time you check it

See IMAGE game.png

here as we move from block 8--> 7---> 6 and so on till ---> 5---> Goal.
So if we check the state of block 7 at any point of time it remains the same <----


*Whereas in case of finite horizons the state changes wrt to time, in case of finite horizons markovian property of a state fails because the policy of the state doesn't remain constant it actually changes with time.

SEE IMAGE game.png

here as we move from block 8--> 7 the algorithm choose to enter block 9 instead of 6 , inspite of the danger of getting into Danger block because this is a finite horizon where the time left to complete the game decerease for every step you take
For example if you have only  4 sec left to complete the game and if you are in block 7 then the game choose the path 7-9-5-goal rather than 7-6-2-3--5-goal.
So the conceptof everything remains stationary fails in case of finite horizon.
hence in case of finite horizon 
Pi(s,t)---> a
t - time

Utility of Sequences: 
if we have two different sequences of states S0,S1,S2..... and S0,S'1,S'2....

if U{S0,S1,S2.....} > U{S0,S'1,S'2....}

then U{S1,S2.....} > U{S'1,S'2....}

where U - utility function

this Utility function is defined as 

U{S0,S1,S2.....} = SUM(from t=0 to Infinity){R(St)} = Infinity.....1.0

ie the value of this utility function is infinity because for any given sequence it rewards is as shown below:

Sequence : S0,S1,S2.....
Reward :   R1,R2,R3...... and so on

so if we keep on adding Rewards R for every sequence ,since seqeunce doesn't end the sum keep on increases and becomes infinity. 

Intutively: It is like whatever you do with your life you'll be rewarded and you'll be immortal, in other words you won't risk to improve yourself because no matter what you do you'll be rewarded.

But if we define U as below

U{S0,S1,S2.....}  = SUM(from t=0 to infinity){Gamma**t R(St)}...2.0

where 0 <= gamma <1
in other words the equation 2.0 is just the sum of terms upto infinity of geometric series, which is

U{S0,S1,S2.....} = (Rmax)* (1 / (1-gamma))....3.0

eq 3.0 is called discounted reward

if we observe 2.0, we can makeout that as t increases gamma value goes on decreasing(since gamma is a fraction less than 1) so the reward at each consecutive step decreases.

It is like covering infinite distance within finite time, that what eq 2.0 means.ie a infinite sum results in a finite value

Why should we discount the rewards?
As we saw from the definition of utility of sequences

U(S0,S1,S2....) = SUM(from t=0 to infinity){R(St)}

ie we are taking the sum of rewards of all the states through which the agent moves in future.but since we are adding infinite number of rewards the sum == infinity.

to make the sum finite we discount it with gamma**t.


Policies:

We know optimal policy is defined as 

Pi* = argmax(Pi){E(R|Pi)}....4.0

 where R = SUM(from t=0 to infinity){gamma**t R(St)}, ie discounted reward

what does equation 4.0 states?
It defines the optimal policy which is nothing but choosing a policy Pi which maximises the longterm expected reward

U{Pi}(S) is U with policy Pi and State S

U{Pi}(S) = E[R|Pi,S0 = S]...5.0

eq 5.0 represents Utility function for a state when you choosen Pi as a Policy. In other words it's indicating the long term rewards that you'll get if you have used the Policy Pi and start at state S0.

or U{Pi}(S) is the utility of the state if the agent starts at S0 = S
and follow the policy Pi.

For example you've choosen a path(Policy) and the start state for your game and you'll play the game,then what would be the reward that you'll be getting at the end of the game is given by eq 5.0 

U(s) always returns delayed or long term rewards whereas R(S) returns the immediate rewards.So rewards and utility can never be the same.


so optimal policy

Pi*(s) = argmax(a){SUM(T(s,a,s')U(s'))}

where 
U(S) = R(S) + Gamma * Max(a){SUM(T(s,a,s')U(s'))}...6.0

eq 6.0 is called Bellman equation


Finding policies:
 we know from Bellman's equation

U(S) = R(S) + Gamma * Max(a){SUM(T(s,a,s')U(s'))} 

so we can use this eq as 

Start with some arbitrary utilities
Update utilities based on neighbors(ie any state that you can reach)
repeat until convergence

^U(S)(t+1) = R(S)  + Gamma Max(a){SUM(T(s,a,s')* ^Ut(s'))}...6.2


So I'll update ^U(S)(t+1), which is the utility of current state 

^Ut(s') is the Utility of all other remaining states.

the equation 6.2 is called value iteration because it starts with random utilities for each state and then it iterates till it converges to true value.Once we have found the true utility then the problem is solved because the optimal policy is defined using true utility values

Pi*(s) = argmax(a){SUM(s'){T(s,a,s')*U(s')}} 

We don't care about the optimal or true utility unless we are able to get Pi* start with existing values of utility function,ie even when we improve the utility function from its current state there if there wouldn't be any change in (optimal)actions for each state, then we need not further optimise our utility function.

Given a utility function we can find the optimal Pi but given a Pi* finding  a unique optimal utility function is not possible since there can be more than one optimal utility functions for the same Pi*

Policy iteration:

method:
1.start with any guess of policy
2.evaluate Pi_t calculate Ut = U_Pi_t
3.improve Pi_t+1 = argmax(a){SUM{T(s,a,s')*U_t(s')}}

U_t(s) = R(s) + gamma * SUM(s'){T(s,Pi_t(s),s')Ut(s')}

SEE IMAGE FP1

Consider the 2 kinds of Reinforcement learning API:

1. Planning:here we provide transitions and rewards as input to the planner which consists of some code, which tries to create a optimal policy from given inputs.This type of policy creation is called Planning

Model ------> PLANNER ---------> Policy
(T,R)				  (Pi)

2. Reinforcement learning: here we pass state(s),action(a),reward(r),next state(s') as inputs from which the code in LEARNER learns the optimal policy.

transitions-----> LEARNER-------> Policy
(s,a,r,s')			   (Pi)


there are two other subprocess of above mentioned APIs
1.MODELER: given transitions it gives out the model

Transitions -----> MODELER -------> Model

2.Simulator: it is like a inverse of a Modeler,it takes model and provides the transitions 

Model ------> SIMULATOR ------> transition

See IMAGE FP2 where 
the planner in 2 consists of value or policy iterations



See IMAGE FP3 where

There are 3 approaches for RL:

1. Policy based RL: they try to find policy,the problem with this approach is that we cannot get the actions to be performed in a given state.

2. Value function based RL: value function maps from states to values .here we try to learn value function.Once we have optimal value function we can get Policy Pi*

3. Model based RL: here we have Transition function(T) and reward function(R), 
where 
R maps from S(present state) to S'(next state)
rewards maps from action to reward

Out of all 3 value function based is the most poweful and simpler one. k

A new kind of value function called Q-function:

Q(s,a) = R(s) + gamma * SUM(s'){T(s,a,s') max(a){Q(s',a')}}...5.6

Eq 5.6 states that value for arriving in S, leaving via action a,proceeding optimally thereafter ie the Q function for state s and action a is = Reward of that state s + gamma * sum of transition prob for moving from state s to state s' via action , times,maximum value of Q function for next state s' and next action a'
 
one advantage of using Q(s,a) is that we can plug in the action a, for each iteration and see how it changes the value of Q function for various states. This helps us to try different values of actions and see their effect instead of algorithm picking these actions

If we have to represent U(s) in terms of Q(s,a) we can write

U(s) = max(a){Q(s,a)}

Why?
Q(s,a) depends on the state and action. So, for each state you have a Q value for each possible action that you can take from that state. U(s) depends only on the state and is the value considering that you have chosen only the "best" actions. So, to find U(s) using Q(s,a), you need to consider that you got the best action on s, that is, the action that leads to the best Q, and U(s) will have the same value as the Q of the best action, and it can be expressed as MAX_a( Q(s,a) ). Is that clear?


1.You have one Q for each possible action in a specific state. Lets consider that you are in state "s1" and you can take actions "a" and "b" in this state. Lets also consider that:

2.f you take action "a" and after that you chose only the "best" actions, you will have a reward of 10

3.If you take action "b" and after that you chose only the "best" actions, you will have a reward of 5

So, if you chose only the best action from state "s1" your reward will be 10, because the best action on "s1" is action "a" as you know that choosing "a" you can get a reward of 10. With this simple example we have:

Q(s1, a) = 10
Q(s1, b) = 5
U(s1) = 10

Is the difference of U and Q clear now? Can you see that U(s) is equal to the Q(s, a) with the maximum value? That is U(s) = MAX_a( Q(s, a) ). It is obvious as you must chose the best action on s1 to get U, and the best action is the one with the larger Q.

And I think that it is now clear also that you have one U for each state, and one Q for each pair of state/action.


Now it's time for policy .We can represent policy in terms of Q(s,a)

P(s) = argmax(a){Q(s,a)}

why?
pi(s) is the optimal policy, given a state it returns the best action to take. The best action will be the action that leads to the best Q. ARGMAX( Q(s,a) ) "returns" the action that leads to the larger Q.

Difference between argmax and max function?

Argmax of a function f(x) is the value of x(input) which maximises f(x), and max of a function is the maxmimum value of the function f(x)(output)

SEE IMAGE FP4

So U(s) = value = max(a){Q(s,a)}

Pi(s) = action = argmax(a){Q(s,a)}

So Q learning is the process of finding optimal Q(s,a)

Q(s,a) = R(s) + gamma *  SUM(s'){T(s,a,s')*max(a){Q(s',a')}}

Practically we won't have the transition values to calculate the Q(s,a)

So we can find Q(s,a) as:

given <s,a,r,s'>

Q^(s,a) <----alpha--- r + gamma* max(a'){Q^(s',a')}

see image FP5

where alpha is the learning rate which helps agent to move by a small amount calculate the reward in that path.


few more things about learning rate:

See IMAGE FP6

here Xt is the value chosen from a distribution with a random variable x. alpha_t is chosen in such a way that sum(alpha_t) = inf and sum(aplha_t^2) < inf.

Why is it "clear that alpha t at each time step is moving closer and closer to zero"?

It is "moving closer" to zero because, in this quiz, alpha was defines as a function of time 't'. Besides that, it must satisfy the two given properties and the second one (sum of (alpha(t))² is a finite value) imply that alpha(t)^2 must tend to zero in the limit. So, intuitively alpha(t) also must tend to zero in the limit. That way, as t grows, alpha get smaller and smaller, "moving closer" to zero. The example, that satisfy both properties, is 1/t. See in the plot of 1/x on google that it goes closer and closer of zero as t grows (is moving to zero):
https://www.google.com.br/search?q=1/x1

And about alpha being a constant, in the previous video, alpha really appears to be defined as a constant, but in the next video, after the quiz, Litman says that in reality he means alpha(t). See:
https://classroom.udacity.com/nanodegrees/nd009/parts/0091345409/modules/e64f9a65-fdb5-4e60-81a9-72813beebb7e/lessons/5446820041/concepts/6348990620923#1

So you could ask, why alpha(t) must satisfy both properties presented in the quiz, right? The answer is that Q Learning is guaranteed to converge to the right solution only if alpha satisfy both properties. You can see that in this video:
https://classroom.udacity.com/nanodegrees/nd009/parts/0091345409/modules/e64f9a65-fdb5-4e60-81a9-72813beebb7e/lessons/5446820041/concepts/63489906309231

-----------------------------------------------------------------
Can anyone please explain why exactly should we use alpha, what is intuition behind using it.

Why should we pick the values of V randomly from X, how does it helps us to find the optimal Q(s,a)?

I also request you to help me understand what the QUiz Learning Incrementally
is all about? I didn't understand the quiz itself

Alpha is the learning rate. It is the percentage that we allow new information to update our current information.

So when we are just starting out we start with a large alpha (because we really don't know anything) but as we have learnt more and more we reduce alpha so that new information doesn't affect us as much as when we didn't know anything.) Sort of like real life.

In the learning incrementally quiz, Vt is sampling from Xt and adding all the samples over time and finding its mean. For a moment assume that we did not have to deal with alpha (i.e alpha = 1). Then in this scenario Vt would be the Expected value of Xt if we had a lot of samples of Xt. Do you agree with this?

Now if we did have alpha (under the equation I just showed above) the value of Vt would still be Expected value of Xt. Now instead of finding a simple mean, we find a weighted average (i.e each new sample effects only fractionally the new value of Vt.)

This is an intuitive explanation. I think the math would be a little complicated.



-------------------------------------------------------------
In the Learning Incrementally Quiz Intro, we're told that xs are defined as being taken from some random distribution X. Then we're told the xs are used to learn the value V (which I think is the best utility value obtainable given the present time t information).

Here's my 2 questions:

1) Is my definition of V good?
2) How randomly picked x can converge to the best value V? Is there any kind of score function that punishes bad move or maybe some kind gradient method is used?


The fact that we're drawing x from a data distribution X means that, as more and more values are drawn from that distribution and are used to update the value V, we can expect that V will converge towards the mean of the distribution, X, which is called the expected value of V. This is actually strikingly similar to the Central Limit Theorem from statistics, and in fact uses the similar definitions:

We can let Y_n from the link be V_t and let the sum be expressed instead as the update rule with alpha_t and x_t. Then you can see that Y_n converges to mu, the mean of the distribution. In the case of this problem, we simply identify it as the expected value.

See IMAGE FP7

here Q^ can be same as the original Q^ with transition prob.But that is not possible because Q^(s',a,s) changes with time



See IMAGE FP8
 this explains the Q-learning convergence.where Q(s,a) is actuall value of Q function

s,a visited infinetly often means that it has to go through all states and action space and we need Transiiton pro and rewards from Reward functions.


Q-learning is not a single algorithm but it is a family of algorithms, these are characterised based on:

1.how do we initialize Q^?
2.how to decay alpha_t?
3.how do we choose actions?


we can combine choosing random actions and using Q^ to take actions,which helps the agent to explore the space/environment.

How?
In this approach most of the time we'll be following Q^,but sometimes the agent takes a random action even if the reward is low, which in turn helps the agent to find the global min when this process is repeated several times, instead of getting stuck in local min.


Epsilon - Greedy exploration: that is initially the agent more random moves but as the time pass number of random moves decays and uses the kowledge what it has more often(Q^).By doing like this

Q^ reaches true Q 
Pi^ reaches true Pi*


Game theory:



















=======================================================================


DEEP LEARNING:
==================

DL excels when we have a massive amount of data and complex problems.DL can be applied easily to wide variety of fields unlike other machine learning methods which are restricted.

Logestic classifier: is a linear classifier of the form

W*X + b = Y

where x =  input(pixels in case of images)
y = ouput(image prediction in case of images)

using softmax function we convert the ouptut values into probabilities

S(yi) = e^(yi)/SUM(over j){e^(yi)}

softmax makes the value of correct label nearer to 1 and others closer to 0.softmax can convert any kind of score to proper probabilities


One-hot encoding is useful when the dependent variable has few classes,but it becomes inefficient when number of class are larger

we use cross-entropy to measure the distance between S(Y) and L
L = one-hot encoded vector = [1,0,0,0,0,.....]

INPUT---> D(S,L)--->one-hot encoding

here input = S(Y) = softmax(y) = 0.7,0.002,0.03..... = whose sum = 1
D(S,L) = - SUM(over i){Li * log(Si)}

D(S,L) != D(L,S)

Together it looks something like ,see Image DP1
this process is called Multinomial Logestic cassification.

Minimizing cross entropy:

we define loss
L = (SUM(over i){D(S(wXi + b)),Li})

to minimize this we find -alpha* delta_L(w,b) ie gradient descent


It is prefered that our features has zero mean and equal variance, because it helps the optimizer to find the optimal value much faster.

See image DP2

in case of images we can normalize features as 

(R-128)/128  	(G-128)/128	(B-128)/128

for weight initialization we use normal distribution with small variance.

Measuring performance:

In most cases min of 30k examples are used for validation test and changes greater than 0.1% are significant for increase in accuracy.But in case of smaller datasets where we cannot save 30k for validation test, we can use cross validation there but it is computationaly expensive, hence it is suggested to collect larger datasets.But this assumption holds good only when all of the classes of dependent variable is same


Stochastic gradient descent: Gradient descent cannot be scaled over larger datasets,because it depends on all the cases or examples of dataset hence its computationaly expensive and slow.Whereas in case Stochastic gradient descent instead of finding gradient descent on entire data set we take random samples from data and we find the gradient descent for that , we repeat this for several samples and using centeral limit theorem we can find the true value.


Momentum technique for stochastic gradient descent(SGD): after the first iteration in SGD we can use the knowledge of in which direction we should move to reach the optimal value using monentum


M = 0.9M + delta_L
See IMage DP3,DP4
DP4 says that we have to decrease the learning rate as move along the iterations


SGD is called Black Magic because
It has many hyper-parameters like:
*initial learning rate
* learning rate decay
* momentum
* batch size
* weight initialization

ADAGRAD is similar to SGD, it implicitly does initial learning rate and learning rate decay and momentum.Using ADAGRAD makes it easy to tune hyperparameters but it acts a bit worse than a precisely tuned SGD

In case of Linear models: we have something called linear model complexity

X	*	W	+	b--------> Y------->S(Y)
N					   K

so we'll have (N+1)*k parameters

Linear models are stable



INTRO TO TENSORFLOW:
==================
Hello, Tensor World!
Let’s analyze the Hello World script you ran. For reference, I’ve added the code below.

import tensorflow as tf

# Create TensorFlow object called hello_constant
hello_constant = tf.constant('Hello World!')

with tf.Session() as sess:
    # Run the tf.constant operation in the session
    output = sess.run(hello_constant)
    print(output)
Tensor
In TensorFlow, data isn’t stored as integers, floats, or strings. These values are encapsulated in an object called a tensor. In the case of hello_constant = tf.constant('Hello World!'), hello_constant is a 0-dimensional string tensor, but tensors come in a variety of sizes as shown below:

# A is a 0-dimensional int32 tensor
A = tf.constant(1234) 
# B is a 1-dimensional int32 tensor
B = tf.constant([123,456,789]) 
 # C is a 2-dimensional int32 tensor
C = tf.constant([ [123,456,789], [222,333,444] ])
tf.constant() is one of many TensorFlow operations you will use in this lesson. The tensor returned by tf.constant() is called a constant tensor, because the value of the tensor never changes.

Session
TensorFlow’s api is built around the idea of a computational graph, a way of visualizing a mathematical process which you learned about in the MiniFlow lesson. Let’s take the TensorFlow code you ran and turn that into a graph:

See IMAGE tf

A "TensorFlow Session", as shown above, is an environment for running a graph. The session is in charge of allocating the operations to GPU(s) and/or CPU(s), including remote machines. Let’s see how you use it.

with tf.Session() as sess:
    output = sess.run(hello_constant)
The code has already created the tensor, hello_constant, from the previous lines. The next step is to evaluate the tensor in a session.

The code creates a session instance, sess, using tf.Session. The sess.run() function then evaluates the tensor and returns the results.

Input
In the last section, you passed a tensor into a session and it returned the result. What if you want to use a non-constant? This is where tf.placeholder() and feed_dict come into place. In this section, you'll go over the basics of feeding data into TensorFlow.

tf.placeholder()
Sadly you can’t just set x to your dataset and put it in TensorFlow, because over time you'll want your TensorFlow model to take in different datasets with different parameters. You need tf.placeholder()!

tf.placeholder() returns a tensor that gets its value from data passed to the tf.session.run() function, allowing you to set the input right before the session runs.

Session’s feed_dict
x = tf.placeholder(tf.string)

with tf.Session() as sess:
    output = sess.run(x, feed_dict={x: 'Hello World'})
Use the feed_dict parameter in tf.session.run() to set the placeholder tensor. The above example shows the tensor x being set to the string "Hello, world". It's also possible to set more than one tensor using feed_dict as shown below.

x = tf.placeholder(tf.string)
y = tf.placeholder(tf.int32)
z = tf.placeholder(tf.float32)

with tf.Session() as sess:
    output = sess.run(x, feed_dict={x: 'Test String', y: 123, z: 45.67})
Note: If the data passed to the feed_dict doesn’t match the tensor type and can’t be cast into the tensor type, you’ll get the error “ValueError: invalid literal for...”.


Input
In the last section, you passed a tensor into a session and it returned the result. What if you want to use a non-constant? This is where tf.placeholder() and feed_dict come into place. In this section, you'll go over the basics of feeding data into TensorFlow.

tf.placeholder()
Sadly you can’t just set x to your dataset and put it in TensorFlow, because over time you'll want your TensorFlow model to take in different datasets with different parameters. You need tf.placeholder()!

tf.placeholder() returns a tensor that gets its value from data passed to the tf.session.run() function, allowing you to set the input right before the session runs.

Session’s feed_dict
x = tf.placeholder(tf.string)

with tf.Session() as sess:
    output = sess.run(x, feed_dict={x: 'Hello World'})
Use the feed_dict parameter in tf.session.run() to set the placeholder tensor. The above example shows the tensor x being set to the string "Hello, world". It's also possible to set more than one tensor using feed_dict as shown below.

x = tf.placeholder(tf.string)
y = tf.placeholder(tf.int32)
z = tf.placeholder(tf.float32)

with tf.Session() as sess:
    output = sess.run(x, feed_dict={x: 'Test String', y: 123, z: 45.67})
Note: If the data passed to the feed_dict doesn’t match the tensor type and can’t be cast into the tensor type, you’ll get the error “ValueError: invalid literal for...”.

TensorFlow Math
Getting the input is great, but now you need to use it. You're going to use basic math functions that everyone knows and loves - add, subtract, multiply, and divide - with tensors. (There's many more math functions you can check out in the documentation.)

Addition
x = tf.add(5, 2)  # 7
You’ll start with the add function. The tf.add() function does exactly what you expect it to do. It takes in two numbers, two tensors, or one of each, and returns their sum as a tensor.

Subtraction and Multiplication
Here’s an example with subtraction and multiplication.

x = tf.subtract(10, 4) # 6
y = tf.multiply(2, 5)  # 10
The x tensor will evaluate to 6, because 10 - 4 = 6. The y tensor will evaluate to 10, because 2 * 5 = 10. That was easy!

Converting types
It may be necessary to convert between types to make certain operators work together. For example, if you tried the following, it would fail with an exception:

tf.subtract(tf.constant(2.0),tf.constant(1))  # Fails with ValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32:
That's because the constant 1 is an integer but the constant 2.0 is a floating point value and subtract expects them to match.

In cases like these, you can either make sure your data is all of the same type, or you can cast a value to another type. In this case, converting the 2.0 to an integer before subtracting, like so, will give the correct result:

tf.subtract(tf.cast(tf.constant(2.0), tf.int32), tf.constant(1))   


Linear functions in TensorFlow
The most common operation in neural networks is calculating the linear combination of inputs, weights, and biases. As a reminder, we can write the output of the linear operation as

Here, W is a matrix of the weights connecting two layers. The output y, the input x, and the biases b are all vectors.
Weights and Bias in TensorFlow
The goal of training a neural network is to modify weights and biases to best predict the labels. In order to use weights and bias, you'll need a Tensor that can be modified. This leaves out tf.placeholder() and tf.constant(), since those Tensors can't be modified. This is where tf.Variable class comes in.
tf.Variable()
x = tf.Variable(5)
The tf.Variable class creates a tensor with an initial value that can be modified, much like a normal Python variable. This tensor stores its state in the session, so you must initialize the state of the tensor manually. You'll use the tf.global_variables_initializer() function to initialize the state of all the Variable tensors.

What is the difference between tf.placeholders and tf.Variable?
Think of Variable in tensorflow as a normal variables which we use in programming languages. We initialize variables, we can modify it later as well. Whereas placeholder doesn’t require initial value. Placeholder simply allocates block of memory for future use. Later, we can use feed_dict to feed the data into placeholder. By default, placeholder has an unconstrained shape, which allows you to feed tensors of different shapes in a session. You can make constrained shape by passing optional argument -shape, as I have done below.

x = tf.placeholder(tf.float32,(3,4))
y =  x + 2
 
sess = tf.Session()
print(sess.run(y)) # will cause an error
 
s = np.random.rand(3,4)
print(sess.run(y, feed_dict={x:s}))
While doing Machine Learning task, most of the time we are unaware of number of rows but (let’s assume) we do know the number of features or columns. In that case, we can use None.

x = tf.placeholder(tf.float32, shape=(None,4))
Now, at run time we can feed any matrix with 4 columns and any number of rows.

Also, Placeholders are used for input data ( they are kind of variables which we use to feed our model), where as Variables are parameters such as weights that we train over time.



Initialization
init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
The tf.global_variables_initializer() call returns an operation that will initialize all TensorFlow variables from the graph. You call the operation using a session to initialize all the variables as shown above. Using the tf.Variable class allows us to change the weights and bias, but an initial value needs to be chosen.

Initializing the weights with random numbers from a normal distribution is good practice. Randomizing the weights helps the model from becoming stuck in the same place every time you train it. You'll learn more about this in the next lesson, when you study gradient descent.

Similarly, choosing weights from a normal distribution prevents any one weight from overwhelming other weights. You'll use the tf.truncated_normal() function to generate random numbers from a normal distribution.

tf.truncated_normal()
n_features = 120
n_labels = 5
weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))
The tf.truncated_normal() function returns a tensor with random values from a normal distribution whose magnitude is no more than 2 standard deviations from the mean.

Since the weights are already helping prevent the model from getting stuck, you don't need to randomize the bias. Let's use the simplest solution, setting the bias to 0.

tf.zeros()
n_labels = 5
bias = tf.Variable(tf.zeros(n_labels))
The tf.zeros() function returns a tensor with all zeros.

TensorFlow Softmax
You might remember in the Intro to TFLearn lesson we used the softmax function to calculate class probabilities as output from the network. The softmax function squashes it's inputs, typically called logits or logit scores, to be between 0 and 1 and also normalizes the outputs such that they all sum to 1. This means the output of the softmax function is equivalent to a categorical probability distribution. It's the perfect function to use as the output activation for a network predicting multiple classes.


see image SF1



TensorFlow Softmax
We're using TensorFlow to build neural networks and, appropriately, there's a function for calculating softmax.

x = tf.nn.softmax([2.0, 1.0, 0.2])
Easy as that! tf.nn.softmax() implements the softmax function for you. It takes in logits and returns softmax activations.



One Hot Encoding
One-Hot Encoding With Scikit-Learn
Transforming your labels into one-hot encoded vectors is pretty simple with scikit-learn using LabelBinarizer. Check it out below!

import numpy as np
from sklearn import preprocessing

# Example labels
labels = np.array([1,5,3,2,1,4,2,1,3])

# Create the encoder
lb = preprocessing.LabelBinarizer()

# Here the encoder finds the classes and assigns one-hot vectors 
lb.fit(labels)

# And finally, transform the labels into one-hot encoded vectors
lb.transform(labels)
>>> array([[1, 0, 0, 0, 0],
           [0, 0, 0, 0, 1],
           [0, 0, 1, 0, 0],
           [0, 1, 0, 0, 0],
           [1, 0, 0, 0, 0],
           [0, 0, 0, 1, 0],
           [0, 1, 0, 0, 0],
           [1, 0, 0, 0, 0],
           [0, 0, 1, 0, 0]])


While using softmax regression ie y = Wx + b, the model converts the 2D image pixel data into 1D vector hence it can take advantage of the 2D structure of image , However other image classifier do consider the 2D image strucutre.

Neural Networks: See IMAGE nn

Implementing the hidden layer:

see image mnn



