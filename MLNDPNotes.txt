*** Source: Bishop-Pattern recognition and Machine learning text book ***


*The ability to categorize correctly new examples that differ from those used for training is known as generalization.

*Feature extraction: is a process of extracting all the vital information(potential features) from inputs to improve the performance of the ML algorithm to predict the output.

Ex: For instance, in the digit recognition problem, the images of the digits are typically translated and scaled so that each digit is contained within a box of a fixed size. This greatly reduces the variability within each digit class, because the location and scale of all the digits are now the same, which makes it much easier for a subsequent pattern recognition algorithm to distinguish between the different classes. 

feature extraction is a part of pre-processing

*Pre-processing might also be performed in order to speed up computation. For example, if the goal is real-time face detection in a high-resolution video stream, the computer must handle huge numbers of pixels per second, and presenting these directly to a complex pattern recognition algorithm may be computationally infeasible. Instead, the aim is to find useful features that are fast to compute, and yet that also preserve useful discriminatory information enabling faces to be distinguished
from non-faces. These features are then used as the inputs to the pattern recognition algorithm.

*Applications in which the training data comprises examples of the input vectors along with their corresponding target vectors are known as supervised learning problems. Cases such as the digit recognition example, in which the aim is to assign each input vector to one of a finite number of discrete categories, are called classification problems. If the desired output consists of one or more continuous variables, then
the task is called regression. An example of a regression problem would be the prediction of the yield in a chemical manufacturing process in which the inputs consist
of the concentrations of reactants, the temperature, and the pressure.

*the technique of reinforcement learning (Sutton and Barto, 1998) is concerned with the problem of finding suitable actions to take in a given situation in
order to maximize a reward. Here the learning algorithm is not given examples of optimal outputs, in contrast to supervised learning, but must instead discover them
by a process of trial and error. 

A general feature of reinforcement learning is the trade-off between exploration, in which the system tries
out new kinds of actions to see how effective they are, and exploitation, in which the system makes use of actions that are known to yield a high reward. Too strong
a focus on either exploration or exploitation will yield poor results.


Probability theory:
i = 1 to M
j = 1 to L

nij = Consider a total of N trials in which we sample both of the variables X and Y , and let the number of such trials in which X = xi and Y = yj be nij.

cij = is the number of trials in which X takes the value xi (irrespective of the value that Y takes)

rij =  is the same case with y

The probability that X will take the value xi and Y will take the value yj is written p(X = xi, Y = yj) and is called the joint probability of X = xi and Y = yj. It is given by the number of points falling in the cell i,j as a fraction of the total number of points, and hence

p(X = xi, Y = yj) = nij/N


Similarly, the probability that X takes the value xi irrespective of the value of Y is written as p(X = xi) and is given by the fraction of the total number of points that fall in column i, so that

p(X = xi) = ci/N

or in general we can write

p(X = xi)  = Sum(j=1 to L){p(X = xi,Y=yj) }


which is the sum rule of probability. Note that p(X = xi) is sometimes called the marginal probability, because it is obtained by marginalizing, or summing out, the
other variables (in this case Y ).


If we consider only those instances for which X = xi, then the fraction of such instances for which Y = yj is written p(Y = yj|X = xi) and is called the conditional probability of Y = yj given X = xi. It is obtained by finding the
fraction of those points in column i that fall in cell i,j and hence is given by


p(Y = yj|X = xi) = nij/ci = P(X&Y)/P(X).

we can then derive the following relationship,which is the product rule
p(X = xi, Y = yj) = nij/N = nij/ci *ci/N = p(Y = yj|X =xi)*p(X = xi)


From the product rule, together with the symmetry property p(X, Y ) = p(Y, X)

p(Y |X) = p(X|Y )p(Y)/p(X)....1.12

which is called Bayes’ theorem and which plays a central role in pattern recognition and machine learning. Using the sum rule, the denominator in Bayes’ theorem can
be expressed in terms of the quantities appearing in the numerator


p(X) = Sum(Y){p(X|Y )p(Y ).}

We can view the denominator in Bayes’ theorem as being the normalization constant required to ensure that the sum of the conditional probability on the left-hand side of
(1.12) over all values of Y equals one.


 we note that if the joint distribution of two variables factorizes into the product of the marginals, so that p(X, Y ) = p(X)p(Y ), then X and Y are said to be independent


From the product rule, we see that p(Y |X) = p(Y ), and so the conditional distribution of Y given X is indeed independent of the value of X


Expectations and covariances:

The average value of some function f(x) under a
probability distribution p(x)(ie for each value of f(x) there will be a value which gives its probability of hapening) is called the expectation of f(x) and will be denoted by E[f]. For a discrete distribution, it is given by


E[F] = Sum(x){P(x)*f(x)}
so that the average is weighted by the relative probabilities of the different values
of x

if we are given a finite number N of points drawn from the probability distribution or probability density, then the expectation can be approximated as a finite sum over these points


E[f] = (1/N)SUM(n=1 to N){f(Xn)}

Sometimes we will be considering expectations of functions of several variables, in which case we can use a subscript to indicate which variable is being averaged over, so that for instance

Ex[f(x, y)]

We can also consider a conditional expectation with respect to a conditional distribution, so that

Ex[f|y] = SUM(x){p(x|y)f(x)}


For two random variables x and y, the covariance is defined by

cov[x, y] = Ex,y [{x − E[x]} {y − E[y]}]
	  = Ex,y[xy] − E[x]E[y]

which expresses the extent to which x and y vary together. If x and y are independent,then their covariance vanishes.

In the case of two vectors of random variables x and y, the covariance is a matrix

cov[x, y] = Ex,y {x − E[x]}{yT − E[yT]}]
	  = Ex,y[xyT] − E[x]E[yT]

If we consider the covariance of the components of a vector x with each other, then we use a slightly simpler notation 

cov[x] ≡ cov[x, x].

 Bayesian probabilities:

We capture our assumptions about w, before observing the data, in the form of a prior probability distribution p(w). The effect of the observed data D = {t1,...,tN} is expressed through the conditional probability p(D|w), 

p(w|D) = p(D|w)p(w)/p(D)


The quantity p(D|w) on the right-hand side of Bayes’ theorem is evaluated for the observed data set D and can be viewed as a function of the parameter vector
w, in which case it is called the likelihood function. It expresses how probable the observed data set is for different settings of the parameter vector w. Note that the
likelihood is not a probability distribution over w, and its integral with respect to w does not (necessarily) equal one. Given this definition of likelihood, we can state Bayes’ theorem in words

posterior ∝ likelihood × prior........1.43

where all of these quantities are viewed as functions of w. The denominator in (1.43) is the normalization constant, which ensures that the posterior distribution
on the left-hand side is a valid probability density and integrates to one.

In both the Bayesian and frequentist paradigms, the likelihood function p(D|w) plays a central role. However, the manner in which it is used is fundamentally different in the two approaches. In a frequentist setting, w is considered to be a fixed parameter, whose value is determined by some form of ‘estimator’, and error bars


on this estimate are obtained by considering the distribution of possible data sets D. By contrast, from the Bayesian viewpoint there is only a single data set D (namely the one that is actually observed), and the uncertainty in the parameters is expressed
through a probability distribution over w


A widely used frequentist estimator is maximum likelihood, in which w is set to the value that maximizes the likelihood function p(D|w). This corresponds to
choosing the value of w for which the probability of the observed data set is maximized. In the machine learning literature, the negative log of the likelihood function
is called an error function. Because the negative logarithm is a monotonically decreasing function, maximizing the likelihood is equivalent to minimizing the error.


One approach to determining frequentist error bars is the bootstrap, in which multiple data sets are created as follows. Suppose our original data set consists of N data points X = {x1, . . . , xN}. We can create a new
data set XB by drawing N points at random from X, with replacement, so that some points in X may be replicated in XB, whereas other points in X may be absent from XB. This process can be repeated L times to generate L data sets each of size N and each obtained by sampling from the original data set X. The statistical accuracy of
parameter estimates can then be evaluated by looking at the variability of predictions between the different bootstrap data sets.

One advantage of the Bayesian viewpoint is that the inclusion of prior knowledge arises naturally. Suppose, for instance, that a fair-looking coin is tossed three
times and lands heads each time. A classical maximum likelihood estimate of the probability of landing heads would give 1, implying that all future tosses will land
heads! By contrast, a Bayesian approach with any reasonable prior will lead to a much less extreme conclusion.


There has been much controversy and debate associated with the relative merits of the frequentist and Bayesian paradigms, which have not been helped by the
fact that there is no unique frequentist, or even Bayesian, viewpoint. For instance, one common criticism of the Bayesian approach is that the prior distribution is often selected on the basis of mathematical convenience rather than as a reflection of any prior beliefs. Even the subjective nature of the conclusions through their dependence on the choice of prior is seen by some as a source of difficulty. Reducing the dependence on the prior is one motivation for so-called noninformative priors. However, these lead to difficulties when comparing different models, and indeed Bayesian methods based on poor choices of prior can give poor results with high
confidence. Frequentist evaluation methods offer some protection from such problems, and techniques such as cross-validation remain useful in areas such as model
comparison.






















*The Gaussian distribution:
For the case of a single real-valued variable x, the Gaussian distribution is defined by

		  1		      -1*(x - µ)^2
N(x|µ, σ^2) =  ----------------  exp{----          }..............5.0
		(2*pi*σ^2)^0.5        2*σ^2


which is governed by two parameters: µ, called the mean, and σ2, called the variance. The square root of the variance, given by σ, is called the standard deviation,
and the reciprocal of the variance, written as β = 1/σ2, is called the precision.


from 5.0 it is clear that N(x|µ, σ^2) > 0

Also it is straightforward to show that the Gaussian is normalized

IMAGE: checkout image GD1,GD2 in images

Now suppose that we have a data set of observations x = (x1, . . . , xN)T, representing N observations of the scalar variable x.We shall suppose that the observations are drawn independently from a Gaussian distribution whose mean µ and variance σ2 are unknown, and we would like to determine these parameters from the data set. Data points that are drawn independently from the same distribution are said to be independent and identically distributed, which is often abbreviated to i.i.d. We have seen that the joint probability of two independent events is given by the product of the marginal probabilities for each event separately. Because our data set x is i.i.d., we can therefore write the probability of the data set, given µ and σ2, in the form

p(x|µ, σ2) = product(from n=1 to n=N){N( Xn|µ, σ^2)}..........5.1

When viewed as a function of µ and σ^2, this is the likelihood function for the Gaussian and is interpreted as 

IMAGE GD3 in images folder

One common criterion for determining the parameters in a probability distribution using an observed data set is to find the parameter values that maximize the likelihood function

we shall determine values for the unknown parameters µ and σ2 in the Gaussian by maximizing the likelihood function in 5.1

 In practice, it is more convenient to maximize the log of the likelihood function. Because the logarithm is a monotonically increasing function of its argument, maximization of the log of a function is equivalent to maximization of the function itself. Taking the log not only simplifies the subsequent mathematical analysis, but it also helps numerically because the product of a large number of small probabilities can easily underflow the numerical precision of the computer, and this is resolved by computing instead the sum of the log probabilities. 

the equation see IMAGE GD4


the significant limitations of the maximum likelihood approach is that the maximum likelihood approach systematically underestimates the variance
of the distribution. This is an example of a phenomenon called bias and is related to the problem of over-fitting encountered in the context of polynomial curve fitting.


IMAGE GD5,GD6

Note that the bias of the maximum likelihood solution becomes less significant as the number N of data points increases, and in the limit N → ∞ the maximum likelihood solution for the variance equals the true variance of the distribution that generated the data. In practice, for anything other than small N, this bias will not prove to be a serious problem. However, throughout this book we shall be interested in more complex models with many parameters, for which the bias problems associated with maximum likelihood will be much more severe. In fact, as we shall see, the issue of bias in maximum likelihood lies at the root of the over-fitting problem that we encountered earlier in the context of polynomial curve fitting.



Curve fitting using basiean approach:(Curve fitting revisited)


We have seen how the problem of polynomial curve fitting can be expressed in terms of error minimization. Here we return to the curve fitting example and view it
from a probabilistic perspective, thereby gaining some insights into error functions and regularization, as well as taking us towards a full Bayesian treatment



The goal in the curve fitting problem is to be able to make predictions for the target variable t given some new value of the input variable x on the basis of a set of
training data comprising N input values x = (x1, . . . , xN)T and their corresponding target values t = (t1, . . . , tN)T. We can express our uncertainty over the value of the target variable using a probability distribution. For this purpose, we shall assume that, given the value of x, the corresponding value of t has a Gaussian distribution with a mean equal to the value y(x, w) of the polynomial curve given by (1.1). Thus we have

p(t|x, w, β) = N(t|y(x, w), β**-1)....... (1.60),
where
y(x, w) = w0 + w1*X + w2*X^2 + . . . + wM*X^M
y(x, w) is the mean
β**-1 = variance = σ^2


see IMAGE GD7

We now use the training data {x, t} to determine the values of the unknown parameters w and β by maximum likelihood. If the data are assumed to be drawn
independently from the distribution (1.60), then the likelihood function is given by

p(t|x, w,β) = product(from n=1 to N){N(tn|y(xn, w),β**−1}.


As we did in the case of the simple Gaussian distribution earlier, it is convenient to maximize the logarithm of the likelihood function. Substituting for the form of the
Gaussian distribution, given by (1.46), we obtain the log likelihood function in the form

ln{p(t|x,w,β)} = -(β/2)* SUM(from n=1 to N){y(xn,w)-tn}^2 + N/2 *ln(β) - N/2 * ln(2*pi).........1.62

 
Consider first the determination of the maximum likelihood solution for the polynomial coefficients, which will be denoted by wML. These are determined by maximizing (1.62) with respect to w. For this purpose, we can omit the last two terms on the right-hand side of (1.62) because they do not depend on w. Also, we note that scaling the log likelihood by a positive constant coefficient does not alter the location of the maximum with respect to w, and so we can replace the coefficient β/2 with 1/2. Finally, instead of maximizing the log likelihood, we can equivalently minimize the negative log likelihood. We therefore see that maximizing likelihood is equivalent, so far as determining w is concerned, to minimizing the sum-of-squares error function defined by 

E(w) = 1/2 * SUM(from n=1 to N){y(xn,w) - tn}^2.....1.2

E(w) = error function or cost function, when this is minimized we get

Thus the sum-of-squares error function has arisen as
a consequence of maximizing likelihood under the assumption of a Gaussian noise distribution.


We can also use maximum likelihood to determine the precision parameter β of the Gaussian conditional distribution. Maximizing (1.62) with respect to β gives

1/βML = 1/N * SUM(from n=1 to N){y(xn, wML) − tn}^2


Having determined the parameters w and β, we can now make predictions for new values of x. Because we now have a probabilistic model, these are expressed in terms of the predictive distribution that gives the probability distribution over t, rather than simply a point estimate, and is obtained by substituting the maximum likelihood parameters(wML,βML) into (1.60) to give

p(t|x, wML, βML) = N(t|y(x, wML), βML**−1).....1.64


Now let us take a step towards a more Bayesian approach and introduce a prior distribution over the polynomial coefficients w. For simplicity, let us consider a
Gaussian distribution of the form

p(w|α) = N(w|0,(α^-1)I) = (α/2*pi)^((M+1)/2) * exp{(−α/2) wTw}...........1.65

where α is alpha, I is the Identity matrix, wT is the transpose of w

where α is the precision of the distribution, and M+1 is the total number of elements in the vector w for an M th order polynomial. Variables such as α, which control
the distribution of model parameters(w,β), are called hyperparameters. Using Bayes’theorem, the posterior distribution for w is proportional to the product of the prior distribution and the likelihood function


p(w|x, t, α, β) ∝ p(t|x, w, β)*p(w|α)....... (1.66)


We can now determine w by finding the most probable value of w given the data, in other words by maximizing the posterior distribution. This technique is called
maximum posterior, or simply MAP. Taking the negative logarithm of (1.66) and combining with (1.62) and (1.65), we find that the maximum of the posterior is given by the minimum of

(β/2) * SUM(from n=1 to N){y(xn,w) - tn}^2 + (α/2) * wTw

Thus we see that maximizing the posterior distribution is equivalent to minimizing the regularized sum-of-squares error function encountered earlier in the form (1.4),
with a regularization parameter given by λ = α/β.


BAYESIAN CURVE FITTING:

Although we have included a prior distribution p(w|α), we are so far still making a point estimate of w and so this does not yet amount to a Bayesian treatment. In
a fully Bayesian approach, we should consistently apply the sum and product rules of probability, which requires, as we shall see shortly, that we integrate over all values of w. Such marginalizations lie at the heart of Bayesian methods for pattern
recognition

In the curve fitting problem, we are given the training data x and t, along with a new test point x, and our goal is to predict the value of t. We therefore wish
to evaluate the predictive distribution p(t|x, x, t). Here we shall assume that the parameters α and β are fixed and known in advance (in later chapters we shall discuss
how such parameters can be inferred from data in a Bayesian setting).

A Bayesian treatment simply corresponds to a consistent application of the sum and product rules of probability, which allow the predictive distribution to be written
in the form


p(t|x, X, t) = INTEGRAL{p(t|x, w)p(w|x, t)dw}.......(1.68)

x- single point
X - vecor of x's

Here p(t|x, w) is given by (1.60), and we have omitted the dependence on α and β to simplify the notation. Here p(w|x, t) is the posterior distribution over parameters, and can be found by normalizing the right-hand side of (1.66). We shall see in Section 3.3 that, for problems such as the curve-fitting example, this posterior distribution is a Gaussian and can be evaluated analytically. Similarly, the integration in (1.68) can also be performed analytically with the result that the predictive distribution is given by a Gaussian of the form


p(t|x, X, t) = N(t|m(x), s^2(x))........... (1.69)

where,s^2(x) = s square of x
where,mean and the variance are given by

m(x) = β * φ(x)T * S * SUM(from n=1 to N){φ(xn) * tn}...1.70

s^2(x) = β**-1 + φ(x)T S φ(x)....1.71

here S**-1 = αI + β SUM(from n=1 to N){ φ(xn) φ(x)T }.......1.72

where I is the unit matrix, and we have defined the vector φ(x) with elements
φi(x) = xi for i = 0, . . . , M


We see that the variance, as well as the mean, of the predictive distribution in (1.69) is dependent on x. The first term in (1.71) represents the uncertainty in the
predicted value of t due to the noise on the target variables and was expressed already in the maximum likelihood predictive distribution (1.64) through βML −1. However, the second term arises from the uncertainty in the parameters w and is a consequence of the Bayesian treatment. The predictive distribution for the synthetic sinusoidal regression problem is illustrated in Figure 1.17.


SEE IMAGE GD8


MODEL SELECTION:
In our example of polynomial curve fitting using least squares, we saw that there was an optimal order of polynomial that gave the best generalization. The order of the polynomial controls the number of free parameters in the model and thereby governs the model complexity. With regularized least squares, the regularization coefficient
λ also controls the effective complexity of the model, whereas for more complex models, such as mixture distributions or neural networks there may be multiple parameters governing complexity. In a practical application, we need to determine the values of such parameters, and the principal objective in doing so is usually to achieve the best predictive performance on new data. Furthermore, as well as finding the appropriate values for complexity parameters within a given model, we may wish to consider a range of different types of model in order to find the best one for
our particular application.


We have already seen that, in the maximum likelihood approach, the performance on the training set is not a good indicator of predictive performance on unseen data due to the problem of over-fitting. If data is plentiful, then one approach is simply to use some of the available data to train a range of models, or a given model
with a range of values for its complexity parameters, and then to compare them on independent data, sometimes called a validation set, and select the one having the
best predictive performance. If the model design is iterated many times using a limited size data set, then some over-fitting to the validation data can occur and so it may be necessary to keep aside a third test set on which the performance of the selected
model is finally evaluated.

In many applications, however, the supply of data for training and testing will be limited, and in order to build good models, we wish to use as much of the available
data as possible for training. However, if the validation set is small, it will give a relatively noisy estimate of predictive performance. One solution to this dilemma is
to use cross-validation, which is illustrated in Figure 1.18. This allows a proportion (S −1)/S of the available data to be used for training while making use of all of the
data to assess performance. When data is particularly scarce, it may be appropriate to consider the case S = N, where N is the total number of data points, which gives
the leave-one-out technique.

SEE IMAGE GD9

One major drawback of cross-validation is that the number of training runs that must be performed is increased by a factor of S, and this can prove problematic for
models in which the training is itself computationally expensive. A further problem with techniques such as cross-validation that use separate data to assess performance
is that we might have multiple complexity parameters for a single model (for instance, there might be several regularization parameters). Exploring combinations
of settings for such parameters could, in the worst case, require a number of training runs that is exponential in the number of parameters. Clearly, we need a better approach. Ideally, this should rely only on the training data and should allow multiple hyperparameters and model types to be compared in a single training run. We therefore need to find a measure of performance which depends only on the training data and which does not suffer from bias due to over-fitting.


The curse of Dimensionality:

As the number of inputs or features increases it poses some serious challenges and is an important factor influencing the design of
pattern recognition techniques.

D = number of inputs

As D increases, so the number of independent coefficients (not all of the coefficients are independent due to interchange symmetries amongst the x variables) grows proportionally to D^3. I

In practice, to capture complex dependencies in the data, we may
need to use a higher-order polynomial. For a polynomial of order M, the growth in the number of coefficients is like D^M . Although this is now a power law growth, rather than an exponential growth, it still points to the method becoming rapidly unwieldy and of limited practical utility.


Although the curse of dimensionality certainly raises important issues for pattern recognition applications, it does not prevent us from finding effective techniques
applicable to high-dimensional spaces. The reasons for this are twofold. First, real data will often be confined to a region of the space having lower effective dimensionality, and in particular the directions over which important variations in the target
variables occur may be so confined. Second, real data will typically exhibit some smoothness properties (at least locally) so that for the most part small changes in the input variables will produce small changes in the target variables, and so we can exploit local interpolation-like techniques to allow us to make predictions of the target variables for new values of the input variables.




*Decision theory:

decision theory, when combined with probability theory,allows us to make optimal decisions in situations involving uncertainty such as those
encountered in pattern recognition.

Decision theory is the subject of decision theory to tell us how to make optimal decisions given the appropriate probabilities. We shall see that the decision stage is generally very simple, even trivial, once we have solved the inference problem.

Before giving a more detailed analysis, let us first consider informally how we might expect probabilities to play a role in making decisions. When we obtain the X-ray image x for a new patient, our goal is to decide which of the two classes to assign to the image. We are interested in the probabilities of the two classes given the image, which are given by p(Ck|x). Using Bayes’ theorem, these probabilities
can be expressed in the form

p(Ck|x) = p(x|Ck)p(Ck)/p(x)........ (1.77)

Note that any of the quantities appearing in Bayes’ theorem can be obtained from the joint distribution p(x, Ck) by either marginalizing or conditioning with respect to the appropriate variables. We can now interpret p(Ck) as the prior probability for the class Ck, and p(Ck|x) as the corresponding posterior probability. Thus p(C1) represents the probability that a person has cancer, before we take the X-ray measurement. Similarly, p(C1|x) is the corresponding probability, revised using Bayes’ theorem in light of the information contained in the X-ray. If our aim is to minimize the chance of assigning x to the wrong class, then intuitively we would choose the class having
the higher posterior probability. We now show that this intuition is correct, and we also discuss more general criteria for making decisions.

	

Minimizing the misclassification rate:

Suppose that our goal is simply to make as few misclassifications as possible.We need a rule that assigns each value of x to one of the available classes. Such a rule will divide the input space into regions Rk called decision regions, one for each class, such that all points in Rk are assigned to class Ck. The boundaries between decision regions are called decision boundaries or decision surfaces. Note that each
decision region need not be contiguous but could comprise some number of disjoint regions. We shall encounter examples of decision boundaries and decision regions in later chapters. In order to find the optimal decision rule, consider first of all the case of two classes, as in the cancer problem for instance. A mistake occurs when an input vector belonging to class C1 is assigned to class C2 or vice versa. The probability of this occurring is given by


p(mistake) = p(x ∈ R1, C2) + p(x ∈ R2, C1)
	   = INTEGRAL(from R1) {p(x, C2)dx} + INTEGRAL(from R2){ p(x, C1)dx}

We are free to choose the decision rule that assigns each point x to one of the two classes. Clearly to minimize p(mistake) we should arrange that each x is assigned to whichever class has the smaller value of the integrand in (1.78). Thus, if p(x, C1) > p(x, C2) for a given value of x, then we should assign that x to class C1. From the
product rule of probability we have p(x, Ck) = p(Ck|x)p(x). Because the factor p(x) is common to both terms, we can restate this result as saying that the minimum


probability of making a mistake is obtained if each value of x is assigned to the class for which the posterior probability p(Ck|x) is largest. This result is illustrated for two classes, and a single input variable x, in Figure 1.24.

IMAGE GD17


For the more general case of K classes, it is slightly easier to maximize the probability of being correct, which is given by

p(correct) = SUM(from k= to K){p(x ∈ Rk, Ck)}

p(correct) = SUM(from k= to K)INTEGRAL(Rk){p(x ∈ Rk, Ck)}


which is maximized when the regions Rk are chosen such that each x is assigned to the class for which p(x, Ck) is largest. Again, using the product rule p(x, Ck) = p(Ck|x)p(x), and noting that the factor of p(x) is common to all terms, we see that each x should be assigned to the class having the largest posterior probability p(Ck|x).

IMAGE GD18


