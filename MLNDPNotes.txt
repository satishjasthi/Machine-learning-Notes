Whenever you are faced with some data, the first step to understand the column of the data and the type of data they posses.And also to check whether data is cleaned properly(ie it may have some NaN value in some columns)

We use panda's .describe() to get the descriptive stats of numerical features of the dataset.

Whenever a column or a feature of a dataset has a Nan value, the best way to deal with it is by replacing NaN with the meadian of the column values.

to do this we can use panda's Series method called .fillna , which takes one argument that it can use to replace the NaN values.

Ex: DataFrameName["column_name"] -- is a series
now,
DataFrameName["column_name"] = DataFrameName["column_name"].fillna(DataFrameName["column_name"].meadian())

In case of categorical features we can find all the unique classes of the features using

np.unique(DataFrameName.Columname)

to convert these categorical feature to a dummy feature

DataFrameName.loc[DataFrameName["ColumnName"] == "class1",ColumnName] =0 

DataFrameName.loc[DataFrameName["ColumnName"] == "class2",ColumnName] = 2

.loc[] is used for label based indexing purpose.

Once the data is cleaned we can use an algo to predic the dependent feature value as shoen below:
----------------------------------------
CODE:
----------------------------------------------------------------------
# Import the linear regression class

from sklearn.linear_model import LinearRegression

# Sklearn also has a helper that makes it easy to do cross validation

from sklearn.cross_validation import KFold

# The columns we'll use to predict the target

predictors = ["Pclass", "Sex", "Age", "SibSp", "Parch", "Fare", "Embarked"]

# Initialize our algorithm class

alg = LinearRegression()

# Generate cross validation folds for the titanic dataset.  It return the row indices corresponding to train and test.
# We set random_state to ensure we get the same splits every time we run this.

kf = KFold(titanic.shape[0], n_folds=3, random_state=1)

predictions = []

for train, test in kf:

    train_predictors = (titanic[predictors].iloc[train,:])

#here you are selecting all the training examples from dataset.
#titanic[predictors] selects all features mentioned in predictors
#titanic[predictors].iloc[train,:], this actually selects all the rows in train variable and all columns of the data set.
# train variable is created during cross validation , it contains the indexs of the values that has to added to training set, like
train =array([1,2,3,4....])

    # The target we're using to train the algorithm.

    train_target = titanic["Survived"].iloc[train]

    # Training the algorithm using the predictors and target.

    alg.fit(train_predictors, train_target)

    # We can now make predictions on the test fold

    test_predictions = alg.predict(titanic[predictors].iloc[test,:])

    predictions.append(test_predictions)

---------------------------------------------------------------------

Sklearn's cross_validation and cross_val_score:
creoss_validation splits the data into k folds whereas cross_val_score calulates the scores for each of these foldes





























